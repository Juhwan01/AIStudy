{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f415aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\daycon\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 디바이스: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import (\n",
    "    T5Tokenizer, T5ForConditionalGeneration,\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from itertools import permutations\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 디바이스: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dac2f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 24개의 순열 클래스 생성\n",
      "예시 매핑:\n",
      "  라벨 0: (0, 1, 2, 3)\n",
      "  라벨 1: (0, 1, 3, 2)\n",
      "  라벨 2: (0, 2, 1, 3)\n",
      "  라벨 3: (0, 2, 3, 1)\n",
      "  라벨 4: (0, 3, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "def create_label_mappings():\n",
    "    \"\"\"24가지 순열에 대한 라벨 매핑 생성\"\"\"\n",
    "    # 가능한 모든 순열 생성 (4! = 24개)\n",
    "    all_permutations = list(permutations([0, 1, 2, 3]))\n",
    "    \n",
    "    # 순열 → 라벨 번호 매핑\n",
    "    perm_to_label = {perm: idx for idx, perm in enumerate(all_permutations)}\n",
    "    \n",
    "    # 라벨 번호 → 순열 매핑 (예측시 사용)\n",
    "    label_to_perm = {idx: perm for idx, perm in enumerate(all_permutations)}\n",
    "    \n",
    "    print(f\"총 {len(all_permutations)}개의 순열 클래스 생성\")\n",
    "    print(\"예시 매핑:\")\n",
    "    for i in range(5):\n",
    "        print(f\"  라벨 {i}: {all_permutations[i]}\")\n",
    "    \n",
    "    return perm_to_label, label_to_perm\n",
    "\n",
    "# 순열 매핑 생성\n",
    "perm_to_label, label_to_perm = create_label_mappings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b59fcb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 전처리 함수 정의 완료!\n"
     ]
    }
   ],
   "source": [
    "def prepare_roberta_data(train_df, perm_to_label):\n",
    "    \"\"\"RoBERTa용 데이터 준비\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for _, row in train_df.iterrows():\n",
    "        # 4개 문장 추출\n",
    "        sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "        \n",
    "        # 정답 순열\n",
    "        answer_tuple = tuple([row[f\"answer_{i}\"] for i in range(4)])\n",
    "        \n",
    "        # 문장들을 [SEP]로 연결\n",
    "        text = \" [SEP] \".join(sentences)\n",
    "        \n",
    "        # 순열을 라벨로 변환\n",
    "        label = perm_to_label[answer_tuple]\n",
    "        \n",
    "        processed_data.append({\n",
    "            \"text\": text,\n",
    "            \"label\": label,\n",
    "            \"original_sentences\": sentences,\n",
    "            \"answer\": answer_tuple\n",
    "        })\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "def make_t5_input(row):\n",
    "    \"\"\"T5용 입력 데이터 생성\"\"\"\n",
    "    sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "    input_text = \"문장을 순서대로 정렬하세요: \" + \" </s> \".join(sentences)\n",
    "    answer = [row[f\"answer_{i}\"] for i in range(4)]\n",
    "    target_text = \" \".join(map(str, answer))  # 예: \"0 3 1 2\"\n",
    "    return {\"input\": input_text, \"target\": target_text}\n",
    "\n",
    "def augment_roberta_data(train_df, perm_to_label, multiplier=3):\n",
    "    \"\"\"데이터 증강으로 학습 데이터 늘리기\"\"\"\n",
    "    augmented_data = []\n",
    "    \n",
    "    # 원본 데이터 처리\n",
    "    original_data = prepare_roberta_data(train_df, perm_to_label)\n",
    "    augmented_data.extend(original_data)\n",
    "    \n",
    "    # 증강 데이터 생성\n",
    "    for _ in range(multiplier - 1):\n",
    "        for _, row in train_df.iterrows():\n",
    "            sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "            original_answer = [row[f\"answer_{i}\"] for i in range(4)]\n",
    "            \n",
    "            # 랜덤하게 문장 순서 섞기\n",
    "            indices = list(range(4))\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            # 새로운 순서의 문장들\n",
    "            shuffled_sentences = [sentences[i] for i in indices]\n",
    "            \n",
    "            # 새로운 정답 계산\n",
    "            new_answer = tuple([indices.index(original_answer[i]) for i in range(4)])\n",
    "            \n",
    "            # 텍스트 생성\n",
    "            text = \" [SEP] \".join(shuffled_sentences)\n",
    "            label = perm_to_label[new_answer]\n",
    "            \n",
    "            augmented_data.append({\n",
    "                \"text\": text,\n",
    "                \"label\": label,\n",
    "                \"original_sentences\": shuffled_sentences,\n",
    "                \"answer\": new_answer\n",
    "            })\n",
    "    \n",
    "    print(f\"데이터 증강 완료: {len(original_data)} → {len(augmented_data)}\")\n",
    "    return augmented_data\n",
    "\n",
    "print(\"데이터 전처리 함수 정의 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4610f3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로드 중...\n",
      "원본 학습 데이터: 7351개\n",
      "\n",
      "데이터 증강 중...\n",
      "데이터 증강 완료: 7351 → 22053\n",
      "\n",
      "학습/검증 데이터 분할...\n",
      "학습 데이터: 17642개\n",
      "검증 데이터: 4411개\n",
      "\n",
      "T5용 데이터 준비 (증강 데이터 사용)...\n",
      "T5 학습 데이터: 17642개 (증강된 데이터)\n",
      "T5 검증 데이터: 4411개 (증강된 데이터)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "print(\"데이터 로드 중...\")\n",
    "train_df = pd.read_csv('./train.csv')\n",
    "print(f\"원본 학습 데이터: {len(train_df)}개\")\n",
    "\n",
    "# 데이터 증강\n",
    "print(\"\\n데이터 증강 중...\")\n",
    "augmented_data = augment_roberta_data(train_df, perm_to_label, multiplier=3)\n",
    "\n",
    "# 학습/검증 분할\n",
    "print(\"\\n학습/검증 데이터 분할...\")\n",
    "train_data, valid_data = train_test_split(\n",
    "    augmented_data, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=[item[\"label\"] for item in augmented_data]\n",
    ")\n",
    "\n",
    "print(f\"학습 데이터: {len(train_data)}개\")\n",
    "print(f\"검증 데이터: {len(valid_data)}개\")\n",
    "\n",
    "# T5용 데이터 준비 (증강 데이터 사용)\n",
    "print(\"\\nT5용 데이터 준비 (증강 데이터 사용)...\")\n",
    "\n",
    "def convert_to_t5_format(augmented_data):\n",
    "    \"\"\"증강된 데이터를 T5 형식으로 변환\"\"\"\n",
    "    t5_data = []\n",
    "    for item in augmented_data:\n",
    "        sentences = item['original_sentences']\n",
    "        answer = item['answer']\n",
    "        \n",
    "        input_text = \"문장을 순서대로 정렬하세요: \" + \" </s> \".join(sentences)\n",
    "        target_text = \" \".join(map(str, answer))\n",
    "        \n",
    "        t5_data.append({\n",
    "            \"input\": input_text,\n",
    "            \"target\": target_text\n",
    "        })\n",
    "    return t5_data\n",
    "\n",
    "# 증강된 train_data, valid_data를 T5 형식으로 변환\n",
    "t5_train_data = convert_to_t5_format(train_data)\n",
    "t5_valid_data = convert_to_t5_format(valid_data)\n",
    "\n",
    "t5_train_dataset = Dataset.from_pandas(pd.DataFrame(t5_train_data))\n",
    "t5_valid_dataset = Dataset.from_pandas(pd.DataFrame(t5_valid_data))\n",
    "\n",
    "print(f\"T5 학습 데이터: {len(t5_train_data)}개 (증강된 데이터)\")\n",
    "print(f\"T5 검증 데이터: {len(t5_valid_data)}개 (증강된 데이터)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b05d8dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 모델 및 토크나이저 로딩...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ T5 모델 로드 완료!\n",
      "T5 데이터셋 토크나이징 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 17642/17642 [00:10<00:00, 1715.35 examples/s]\n",
      "Map: 100%|██████████| 4411/4411 [00:02<00:00, 1715.26 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 토크나이징 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"T5 모델 및 토크나이저 로딩...\")\n",
    "\n",
    "# T5 토크나이저 및 모델 로딩\n",
    "t5_model_name = \"t5-small\"\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name, cache_dir='C:/huggingface_cache')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name, cache_dir='C:/huggingface_cache')\n",
    "t5_model.to(device)\n",
    "\n",
    "print(\"✅ T5 모델 로드 완료!\")\n",
    "\n",
    "# T5 토크나이징 함수\n",
    "def tokenize_t5(example):\n",
    "    model_inputs = t5_tokenizer(example[\"input\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = t5_tokenizer(example[\"target\"], max_length=16, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# T5 데이터셋 토크나이징\n",
    "print(\"T5 데이터셋 토크나이징 중...\")\n",
    "t5_tokenized_train = t5_train_dataset.map(tokenize_t5, batched=True)\n",
    "t5_tokenized_valid = t5_valid_dataset.map(tokenize_t5, batched=True)\n",
    "print(\"T5 토크나이징 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25f9c966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 모델 학습 시작...\n",
      "T5 학습 시작...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7600' max='16545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7600/16545 1:26:32 < 1:41:52, 1.46 it/s, Epoch 6/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.200400</td>\n",
       "      <td>0.183419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.198700</td>\n",
       "      <td>0.185852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.195400</td>\n",
       "      <td>0.180419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.194100</td>\n",
       "      <td>0.174857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.187600</td>\n",
       "      <td>0.171759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.180800</td>\n",
       "      <td>0.170843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.181800</td>\n",
       "      <td>0.170376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.179600</td>\n",
       "      <td>0.170007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.179800</td>\n",
       "      <td>0.169599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.175500</td>\n",
       "      <td>0.166112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.174900</td>\n",
       "      <td>0.163657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.173400</td>\n",
       "      <td>0.160670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.171200</td>\n",
       "      <td>0.160151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.172500</td>\n",
       "      <td>0.156963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.155784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.164200</td>\n",
       "      <td>0.158945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.153827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.161800</td>\n",
       "      <td>0.155321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>0.153812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.164800</td>\n",
       "      <td>0.153249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.158300</td>\n",
       "      <td>0.152927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.152994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.154555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.154700</td>\n",
       "      <td>0.151874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.160900</td>\n",
       "      <td>0.151269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.154900</td>\n",
       "      <td>0.153947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.158300</td>\n",
       "      <td>0.153994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.154700</td>\n",
       "      <td>0.150499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.156900</td>\n",
       "      <td>0.149693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.155900</td>\n",
       "      <td>0.149324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.156900</td>\n",
       "      <td>0.147301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.154600</td>\n",
       "      <td>0.146427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.156000</td>\n",
       "      <td>0.148685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.154100</td>\n",
       "      <td>0.146496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.158000</td>\n",
       "      <td>0.144943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.154400</td>\n",
       "      <td>0.148164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.147644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.152600</td>\n",
       "      <td>0.146666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7600, training_loss=0.16754526602594477, metrics={'train_runtime': 5206.008, 'train_samples_per_second': 50.832, 'train_steps_per_second': 3.178, 'total_flos': 1.6451066652524544e+16, 'train_loss': 0.16754526602594477, 'epoch': 6.890702947845805})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"T5 모델 학습 시작...\")\n",
    "\n",
    "# EarlyStoppingCallback 추가\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# T5 학습 설정 (RTX 2070 Super 8GB 최적화)\n",
    "t5_training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_results\",\n",
    "    learning_rate=3e-5,  # 조금 높여서 빠른 수렴\n",
    "    \n",
    "    # 메모리 최적화 배치 설정\n",
    "    per_device_train_batch_size=4,  # 16 → 4로 대폭 축소\n",
    "    per_device_eval_batch_size=8,   # 32 → 8로 축소\n",
    "    gradient_accumulation_steps=4,  # 실제 배치 크기: 4*4=16\n",
    "    \n",
    "    num_train_epochs=15,  # 20 → 15로 단축\n",
    "    \n",
    "    # 메모리 최적화 설정\n",
    "    fp16=True,  # 메모리 50% 절약\n",
    "    gradient_checkpointing=True,  # 메모리 절약 (속도 20% 감소)\n",
    "    dataloader_pin_memory=False,  # 메모리 절약\n",
    "    dataloader_num_workers=2,     # 4 → 2로 축소\n",
    "    \n",
    "    # 평가 및 저장 (조기 종료를 위해 더 자주)\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,  # 100 → 200으로 늘려서 메모리 압박 완화\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,  # 3 → 2로 축소 (디스크 공간 절약)\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # 로깅\n",
    "    logging_steps=100,\n",
    "    report_to=None,\n",
    "    \n",
    "    # 추가 메모리 최적화\n",
    "    remove_unused_columns=True,\n",
    "    dataloader_drop_last=True,\n",
    ")\n",
    "\n",
    "# T5 Trainer 정의 (EarlyStoppingCallback 추가)\n",
    "t5_trainer = Trainer(\n",
    "    model=t5_model,\n",
    "    args=t5_training_args,\n",
    "    train_dataset=t5_tokenized_train,\n",
    "    eval_dataset=t5_tokenized_valid,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # 3번 연속 개선 없으면 중단\n",
    ")\n",
    "\n",
    "print(\"T5 학습 시작...\")\n",
    "t5_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8815f19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 모델 저장 중...\n",
      "✅ T5 학습 및 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "# T5 모델 저장\n",
    "print(\"T5 모델 저장 중...\")\n",
    "t5_trainer.save_model(\"./t5_results/final_model\")\n",
    "t5_tokenizer.save_pretrained(\"./t5_results/final_model\")\n",
    "print(\"✅ T5 학습 및 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53c262f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa 모델 및 토크나이저 로딩...\n",
      "토크나이저 로드 성공: BertTokenizerFast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RoBERTa 모델 로드 완료!\n",
      "RoBERTa 데이터셋 토크나이징 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 17642/17642 [00:02<00:00, 8513.26 examples/s]\n",
      "Map: 100%|██████████| 4411/4411 [00:00<00:00, 8505.29 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa 토크나이징 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"RoBERTa 모델 및 토크나이저 로딩...\")\n",
    "\n",
    "# RoBERTa 모델 설정\n",
    "roberta_model_name = \"klue/roberta-base\"\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_name, cache_dir='C:/huggingface_cache')\n",
    "print(f\"토크나이저 로드 성공: {type(roberta_tokenizer).__name__}\")\n",
    "\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    roberta_model_name,\n",
    "    num_labels=24,  # 24가지 순열\n",
    "    cache_dir='C:/huggingface_cache'\n",
    ")\n",
    "roberta_model.to(device)\n",
    "\n",
    "print(\"✅ RoBERTa 모델 로드 완료!\")\n",
    "\n",
    "# RoBERTa 데이터셋 준비\n",
    "train_df_processed = pd.DataFrame(train_data)\n",
    "valid_df_processed = pd.DataFrame(valid_data)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df_processed)\n",
    "valid_dataset = Dataset.from_pandas(valid_df_processed)\n",
    "\n",
    "# RoBERTa 토크나이징 함수\n",
    "def tokenize_roberta(examples):\n",
    "    return roberta_tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# RoBERTa 데이터셋 토크나이징\n",
    "print(\"RoBERTa 데이터셋 토크나이징 중...\")\n",
    "roberta_tokenized_train = train_dataset.map(tokenize_roberta, batched=True)\n",
    "roberta_tokenized_valid = valid_dataset.map(tokenize_roberta, batched=True)\n",
    "\n",
    "# 불필요한 컬럼 제거\n",
    "roberta_tokenized_train = roberta_tokenized_train.remove_columns([\"text\", \"original_sentences\", \"answer\"])\n",
    "roberta_tokenized_valid = roberta_tokenized_valid.remove_columns([\"text\", \"original_sentences\", \"answer\"])\n",
    "\n",
    "print(\"RoBERTa 토크나이징 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11914a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa 모델 학습 시작...\n",
      "RoBERTa 학습 시작...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4140' max='4140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4140/4140 46:42, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.580900</td>\n",
       "      <td>2.467889</td>\n",
       "      <td>0.122719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.822300</td>\n",
       "      <td>1.577760</td>\n",
       "      <td>0.438412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.826100</td>\n",
       "      <td>0.842864</td>\n",
       "      <td>0.667427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.588100</td>\n",
       "      <td>0.545402</td>\n",
       "      <td>0.818203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.388300</td>\n",
       "      <td>0.419764</td>\n",
       "      <td>0.868385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.221400</td>\n",
       "      <td>0.336767</td>\n",
       "      <td>0.907162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.175700</td>\n",
       "      <td>0.303742</td>\n",
       "      <td>0.919024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.152400</td>\n",
       "      <td>0.263528</td>\n",
       "      <td>0.929516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.103400</td>\n",
       "      <td>0.216563</td>\n",
       "      <td>0.943203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.219737</td>\n",
       "      <td>0.950730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.062800</td>\n",
       "      <td>0.168223</td>\n",
       "      <td>0.962819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.164478</td>\n",
       "      <td>0.966925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>0.167992</td>\n",
       "      <td>0.968066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>0.177671</td>\n",
       "      <td>0.964416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>0.154645</td>\n",
       "      <td>0.971943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.145898</td>\n",
       "      <td>0.974909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.150761</td>\n",
       "      <td>0.972400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.145341</td>\n",
       "      <td>0.974224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.138861</td>\n",
       "      <td>0.976049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.139987</td>\n",
       "      <td>0.975593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4140, training_loss=0.405164846583553, metrics={'train_runtime': 2824.0126, 'train_samples_per_second': 93.707, 'train_steps_per_second': 1.466, 'total_flos': 1.9441645636299264e+16, 'train_loss': 0.405164846583553, 'epoch': 15.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"RoBERTa 모델 학습 시작...\")\n",
    "\n",
    "# EarlyStoppingCallback import (이미 위에서 했지만 안전하게)\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# 평가 메트릭 정의\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# RoBERTa 학습 설정 (RTX 2070 Super 8GB 최적화)\n",
    "roberta_training_args = TrainingArguments(\n",
    "    output_dir=\"./roberta_results\",\n",
    "    learning_rate=3e-5,  # 조금 높여서 빠른 수렴\n",
    "    \n",
    "    # 메모리 최적화 배치 설정\n",
    "    per_device_train_batch_size=16,  # 64 → 16로 축소\n",
    "    per_device_eval_batch_size=32,   # 128 → 32로 축소  \n",
    "    gradient_accumulation_steps=4,   # 2 → 4로 증가, 실제 배치: 16*4=64\n",
    "    \n",
    "    num_train_epochs=15,  # 20 → 15로 단축\n",
    "    \n",
    "    # 메모리 최적화 설정\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,  # 메모리 절약\n",
    "    dataloader_pin_memory=False,  # 메모리 절약\n",
    "    dataloader_num_workers=2,     # 6 → 2로 축소\n",
    "    \n",
    "    # 평가 및 저장 (조기 종료를 위해 더 자주)\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,  # 100 → 200으로 늘려서 메모리 압박 완화\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,  # 3 → 2로 축소\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_accuracy\",  # 정확도 기준\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # 학습 최적화\n",
    "    warmup_steps=200,  # 150 → 200\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # 로깅\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,  # 20 → 50\n",
    "    report_to=None,\n",
    "    \n",
    "    # 추가 메모리 최적화\n",
    "    remove_unused_columns=True,\n",
    "    dataloader_drop_last=True,\n",
    ")\n",
    "\n",
    "# 데이터 콜레이터\n",
    "data_collator = DataCollatorWithPadding(tokenizer=roberta_tokenizer)\n",
    "\n",
    "# RoBERTa Trainer 설정 (EarlyStoppingCallback 추가)\n",
    "roberta_trainer = Trainer(\n",
    "    model=roberta_model,\n",
    "    args=roberta_training_args,\n",
    "    train_dataset=roberta_tokenized_train,\n",
    "    eval_dataset=roberta_tokenized_valid,\n",
    "    tokenizer=roberta_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # 3번 연속 개선 없으면 중단\n",
    ")\n",
    "\n",
    "print(\"RoBERTa 학습 시작...\")\n",
    "roberta_trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27968784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa 모델 저장 중...\n",
      "✅ RoBERTa 학습 및 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "# RoBERTa 모델 저장\n",
    "print(\"RoBERTa 모델 저장 중...\")\n",
    "roberta_trainer.save_model(\"./roberta_results/final_model\")\n",
    "roberta_tokenizer.save_pretrained(\"./roberta_results/final_model\")\n",
    "print(\"✅ RoBERTa 학습 및 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55fcb696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습된 모델들 로드 중...\n",
      "학습된 T5 모델 로딩...\n",
      "✅ 학습된 T5 모델 로드 완료!\n",
      "학습된 RoBERTa 모델 로딩...\n",
      "✅ 학습된 RoBERTa 모델 로드 완료!\n",
      "\n",
      "🎉 모든 모델 학습 및 로드 완료!\n",
      "GPU 메모리: 3.05GB 사용 / 8.00GB 전체 (38.1% 사용)\n"
     ]
    }
   ],
   "source": [
    "print(\"학습된 모델들 로드 중...\")\n",
    "\n",
    "# 학습된 T5 모델 로드\n",
    "print(\"학습된 T5 모델 로딩...\")\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"./t5_results/final_model\")\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"./t5_results/final_model\")\n",
    "t5_model.to(device)\n",
    "t5_model.eval()\n",
    "print(\"✅ 학습된 T5 모델 로드 완료!\")\n",
    "\n",
    "# 학습된 RoBERTa 모델 로드\n",
    "print(\"학습된 RoBERTa 모델 로딩...\")\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"./roberta_results/final_model\")\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(\"./roberta_results/final_model\")\n",
    "roberta_model.to(device)\n",
    "roberta_model.eval()\n",
    "print(\"✅ 학습된 RoBERTa 모델 로드 완료!\")\n",
    "\n",
    "print(\"\\n🎉 모든 모델 학습 및 로드 완료!\")\n",
    "\n",
    "# GPU 메모리 체크\n",
    "def print_gpu_utilization():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        usage_percent = (allocated / total) * 100\n",
    "        print(f\"GPU 메모리: {allocated:.2f}GB 사용 / {total:.2f}GB 전체 ({usage_percent:.1f}% 사용)\")\n",
    "    else:\n",
    "        print(\"CUDA 사용 불가\")\n",
    "\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f386838d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "앙상블 예측 함수 정의 완료!\n"
     ]
    }
   ],
   "source": [
    "def predict_t5(sentences, model, tokenizer, device):\n",
    "    \"\"\"T5 모델로 예측\"\"\"\n",
    "    input_text = \"문장을 순서대로 정렬하세요: \" + \" </s> \".join(sentences)\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_length=16,\n",
    "            do_sample=True,      \n",
    "            temperature=0.2,     \n",
    "            top_p=0.9,            \n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    try:\n",
    "        order = list(map(int, decoded.strip().split()))\n",
    "        if len(order) == 4 and set(order) == {0, 1, 2, 3}:\n",
    "            return order, 1.0\n",
    "        else:\n",
    "            return [0, 1, 2, 3], 0.1\n",
    "    except:\n",
    "        return [0, 1, 2, 3], 0.1\n",
    "\n",
    "def predict_roberta(sentences, model, tokenizer, label_to_perm, device):\n",
    "    \"\"\"RoBERTa 모델로 예측\"\"\"\n",
    "    text = \" [SEP] \".join(sentences)\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_label = torch.argmax(predictions, dim=-1).item()\n",
    "        confidence = predictions[0][predicted_label].item()\n",
    "    \n",
    "    predicted_order = list(label_to_perm[predicted_label])\n",
    "    return predicted_order, confidence\n",
    "\n",
    "def ensemble_predict_confidence_based(sentences, t5_model, t5_tokenizer, roberta_model, roberta_tokenizer, label_to_perm, device, confidence_threshold=0.7):\n",
    "    \"\"\"신뢰도 기반 앙상블 예측\"\"\"\n",
    "    # 각 모델의 예측\n",
    "    t5_pred, t5_conf = predict_t5(sentences, t5_model, t5_tokenizer, device)\n",
    "    roberta_pred, roberta_conf = predict_roberta(sentences, roberta_model, roberta_tokenizer, label_to_perm, device)\n",
    "    \n",
    "    # RoBERTa 신뢰도가 임계값보다 높으면 RoBERTa 선택\n",
    "    if roberta_conf >= confidence_threshold:\n",
    "        return roberta_pred, roberta_conf, \"roberta\"\n",
    "    else:\n",
    "        return t5_pred, t5_conf, \"t5\"\n",
    "\n",
    "print(\"앙상블 예측 함수 정의 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02b3226e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "앙상블 성능 평가 중... (신뢰도 임계값: 0.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 진행: 100%|██████████| 4411/4411 [05:49<00:00, 12.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 앙상블 평가 결과 ===\n",
      "정확도: 0.9739 (97.39%)\n",
      "정답 개수: 4296/4411\n",
      "평균 신뢰도: 0.9967\n",
      "모델 선택 통계: {'roberta': 4391, 't5': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_ensemble_performance(valid_data, confidence_threshold=0.7):\n",
    "    \"\"\"앙상블 성능 평가\"\"\"\n",
    "    correct = 0\n",
    "    total = len(valid_data)\n",
    "    \n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    model_selections = []\n",
    "    \n",
    "    print(f\"앙상블 성능 평가 중... (신뢰도 임계값: {confidence_threshold})\")\n",
    "    \n",
    "    for example in tqdm(valid_data, desc=\"평가 진행\"):\n",
    "        sentences = example['original_sentences']\n",
    "        true_order = list(example['answer'])\n",
    "        \n",
    "        pred_order, confidence, selected_model = ensemble_predict_confidence_based(\n",
    "            sentences, t5_model, t5_tokenizer, roberta_model, roberta_tokenizer, \n",
    "            label_to_perm, device, confidence_threshold\n",
    "        )\n",
    "        \n",
    "        predictions.append(pred_order)\n",
    "        confidences.append(confidence)\n",
    "        model_selections.append(selected_model)\n",
    "        \n",
    "        if pred_order == true_order:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    avg_confidence = np.mean(confidences)\n",
    "    \n",
    "    # 모델 선택 통계\n",
    "    model_stats = Counter(model_selections)\n",
    "    \n",
    "    print(f\"\\n=== 앙상블 평가 결과 ===\")\n",
    "    print(f\"정확도: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"정답 개수: {correct}/{total}\")\n",
    "    print(f\"평균 신뢰도: {avg_confidence:.4f}\")\n",
    "    print(f\"모델 선택 통계: {dict(model_stats)}\")\n",
    "    \n",
    "    return accuracy, predictions, confidences, model_selections\n",
    "\n",
    "# 성능 평가 실행\n",
    "accuracy, val_predictions, val_confidences, val_selections = evaluate_ensemble_performance(valid_data, confidence_threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e96679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 예측 중...\n",
      "테스트 데이터: 1780개\n",
      "사용할 신뢰도 임계값: 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "앙상블 예측 진행: 100%|██████████| 1780/1780 [02:23<00:00, 12.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 테스트 예측 완료 ===\n",
      "평균 예측 신뢰도: 0.9917\n",
      "모델 선택 통계: {'roberta': 1755, 't5': 25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 데이터 예측 중...\")\n",
    "\n",
    "# 테스트 데이터 로드\n",
    "test_df = pd.read_csv(\"./test.csv\")\n",
    "print(f\"테스트 데이터: {len(test_df)}개\")\n",
    "\n",
    "predictions = []\n",
    "confidences = []\n",
    "model_selections = []\n",
    "\n",
    "# 신뢰도 임계값 설정\n",
    "optimal_threshold = 0.8\n",
    "print(f\"사용할 신뢰도 임계값: {optimal_threshold}\")\n",
    "\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"앙상블 예측 진행\"):\n",
    "    sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "    \n",
    "    predicted_order, confidence, selected_model = ensemble_predict_confidence_based(\n",
    "        sentences, t5_model, t5_tokenizer, roberta_model, roberta_tokenizer, \n",
    "        label_to_perm, device, optimal_threshold\n",
    "    )\n",
    "    \n",
    "    predictions.append(predicted_order)\n",
    "    confidences.append(confidence)\n",
    "    model_selections.append(selected_model)\n",
    "\n",
    "# 예측 통계\n",
    "avg_confidence = np.mean(confidences)\n",
    "model_stats = Counter(model_selections)\n",
    "\n",
    "print(f\"\\n=== 테스트 예측 완료 ===\")\n",
    "print(f\"평균 예측 신뢰도: {avg_confidence:.4f}\")\n",
    "print(f\"모델 선택 통계: {dict(model_stats)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d00ba59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission 파일 생성 중...\n",
      "제출 파일 저장 완료: ensemble_t5_roberta_finetuned.csv\n",
      "\n",
      "============================================================\n",
      "🎯 T5 + RoBERTa Fine-tuned 앙상블 완료!\n",
      "============================================================\n",
      "📊 검증 정확도: 0.9739 (97.39%)\n",
      "🔮 평균 예측 신뢰도: 99.17%\n",
      "🎚️ 신뢰도 임계값: 0.7\n",
      "🤖 테스트셋 모델 선택: {'roberta': 1755, 't5': 25}\n",
      "📁 제출 파일: ensemble_t5_roberta_finetuned.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Submission 파일 생성 중...\")\n",
    "\n",
    "# 샘플 제출 파일 로드\n",
    "sample_submission = pd.read_csv(\"./sample_submission.csv\")\n",
    "\n",
    "# 예측 결과를 제출 형식으로 변환\n",
    "for i in range(4):\n",
    "    sample_submission[f\"answer_{i}\"] = [pred[i] for pred in predictions]\n",
    "\n",
    "# 파일 저장\n",
    "submission_filename = \"ensemble_t5_roberta_finetuned.csv\"\n",
    "sample_submission.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"제출 파일 저장 완료: {submission_filename}\")\n",
    "\n",
    "# 최종 결과 요약\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"🎯 T5 + RoBERTa Fine-tuned 앙상블 완료!\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"📊 검증 정확도: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"🔮 평균 예측 신뢰도: {avg_confidence*100:.2f}%\")\n",
    "print(f\"🎚️ 신뢰도 임계값: {optimal_threshold}\")\n",
    "print(f\"🤖 테스트셋 모델 선택: {dict(model_stats)}\")\n",
    "print(f\"📁 제출 파일: {submission_filename}\")\n",
    "print(f\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb574eec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daycon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
