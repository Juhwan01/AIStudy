{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f415aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\daycon\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import (\n",
    "    T5Tokenizer, T5ForConditionalGeneration,\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from itertools import permutations\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dac2f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 24ê°œì˜ ìˆœì—´ í´ë˜ìŠ¤ ìƒì„±\n",
      "ì˜ˆì‹œ ë§¤í•‘:\n",
      "  ë¼ë²¨ 0: (0, 1, 2, 3)\n",
      "  ë¼ë²¨ 1: (0, 1, 3, 2)\n",
      "  ë¼ë²¨ 2: (0, 2, 1, 3)\n",
      "  ë¼ë²¨ 3: (0, 2, 3, 1)\n",
      "  ë¼ë²¨ 4: (0, 3, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "def create_label_mappings():\n",
    "    \"\"\"24ê°€ì§€ ìˆœì—´ì— ëŒ€í•œ ë¼ë²¨ ë§¤í•‘ ìƒì„±\"\"\"\n",
    "    # ê°€ëŠ¥í•œ ëª¨ë“  ìˆœì—´ ìƒì„± (4! = 24ê°œ)\n",
    "    all_permutations = list(permutations([0, 1, 2, 3]))\n",
    "    \n",
    "    # ìˆœì—´ â†’ ë¼ë²¨ ë²ˆí˜¸ ë§¤í•‘\n",
    "    perm_to_label = {perm: idx for idx, perm in enumerate(all_permutations)}\n",
    "    \n",
    "    # ë¼ë²¨ ë²ˆí˜¸ â†’ ìˆœì—´ ë§¤í•‘ (ì˜ˆì¸¡ì‹œ ì‚¬ìš©)\n",
    "    label_to_perm = {idx: perm for idx, perm in enumerate(all_permutations)}\n",
    "    \n",
    "    print(f\"ì´ {len(all_permutations)}ê°œì˜ ìˆœì—´ í´ë˜ìŠ¤ ìƒì„±\")\n",
    "    print(\"ì˜ˆì‹œ ë§¤í•‘:\")\n",
    "    for i in range(5):\n",
    "        print(f\"  ë¼ë²¨ {i}: {all_permutations[i]}\")\n",
    "    \n",
    "    return perm_to_label, label_to_perm\n",
    "\n",
    "# ìˆœì—´ ë§¤í•‘ ìƒì„±\n",
    "perm_to_label, label_to_perm = create_label_mappings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b59fcb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "def prepare_roberta_data(train_df, perm_to_label):\n",
    "    \"\"\"RoBERTaìš© ë°ì´í„° ì¤€ë¹„\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for _, row in train_df.iterrows():\n",
    "        # 4ê°œ ë¬¸ì¥ ì¶”ì¶œ\n",
    "        sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "        \n",
    "        # ì •ë‹µ ìˆœì—´\n",
    "        answer_tuple = tuple([row[f\"answer_{i}\"] for i in range(4)])\n",
    "        \n",
    "        # ë¬¸ì¥ë“¤ì„ [SEP]ë¡œ ì—°ê²°\n",
    "        text = \" [SEP] \".join(sentences)\n",
    "        \n",
    "        # ìˆœì—´ì„ ë¼ë²¨ë¡œ ë³€í™˜\n",
    "        label = perm_to_label[answer_tuple]\n",
    "        \n",
    "        processed_data.append({\n",
    "            \"text\": text,\n",
    "            \"label\": label,\n",
    "            \"original_sentences\": sentences,\n",
    "            \"answer\": answer_tuple\n",
    "        })\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "def make_t5_input(row):\n",
    "    \"\"\"T5ìš© ì…ë ¥ ë°ì´í„° ìƒì„±\"\"\"\n",
    "    sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "    input_text = \"ë¬¸ì¥ì„ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì„¸ìš”: \" + \" </s> \".join(sentences)\n",
    "    answer = [row[f\"answer_{i}\"] for i in range(4)]\n",
    "    target_text = \" \".join(map(str, answer))  # ì˜ˆ: \"0 3 1 2\"\n",
    "    return {\"input\": input_text, \"target\": target_text}\n",
    "\n",
    "def augment_roberta_data(train_df, perm_to_label, multiplier=3):\n",
    "    \"\"\"ë°ì´í„° ì¦ê°•ìœ¼ë¡œ í•™ìŠµ ë°ì´í„° ëŠ˜ë¦¬ê¸°\"\"\"\n",
    "    augmented_data = []\n",
    "    \n",
    "    # ì›ë³¸ ë°ì´í„° ì²˜ë¦¬\n",
    "    original_data = prepare_roberta_data(train_df, perm_to_label)\n",
    "    augmented_data.extend(original_data)\n",
    "    \n",
    "    # ì¦ê°• ë°ì´í„° ìƒì„±\n",
    "    for _ in range(multiplier - 1):\n",
    "        for _, row in train_df.iterrows():\n",
    "            sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "            original_answer = [row[f\"answer_{i}\"] for i in range(4)]\n",
    "            \n",
    "            # ëœë¤í•˜ê²Œ ë¬¸ì¥ ìˆœì„œ ì„ê¸°\n",
    "            indices = list(range(4))\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            # ìƒˆë¡œìš´ ìˆœì„œì˜ ë¬¸ì¥ë“¤\n",
    "            shuffled_sentences = [sentences[i] for i in indices]\n",
    "            \n",
    "            # ìƒˆë¡œìš´ ì •ë‹µ ê³„ì‚°\n",
    "            new_answer = tuple([indices.index(original_answer[i]) for i in range(4)])\n",
    "            \n",
    "            # í…ìŠ¤íŠ¸ ìƒì„±\n",
    "            text = \" [SEP] \".join(shuffled_sentences)\n",
    "            label = perm_to_label[new_answer]\n",
    "            \n",
    "            augmented_data.append({\n",
    "                \"text\": text,\n",
    "                \"label\": label,\n",
    "                \"original_sentences\": shuffled_sentences,\n",
    "                \"answer\": new_answer\n",
    "            })\n",
    "    \n",
    "    print(f\"ë°ì´í„° ì¦ê°• ì™„ë£Œ: {len(original_data)} â†’ {len(augmented_data)}\")\n",
    "    return augmented_data\n",
    "\n",
    "print(\"ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4610f3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ë¡œë“œ ì¤‘...\n",
      "ì›ë³¸ í•™ìŠµ ë°ì´í„°: 7351ê°œ\n",
      "\n",
      "ë°ì´í„° ì¦ê°• ì¤‘...\n",
      "ë°ì´í„° ì¦ê°• ì™„ë£Œ: 7351 â†’ 22053\n",
      "\n",
      "í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• ...\n",
      "í•™ìŠµ ë°ì´í„°: 17642ê°œ\n",
      "ê²€ì¦ ë°ì´í„°: 4411ê°œ\n",
      "\n",
      "T5ìš© ë°ì´í„° ì¤€ë¹„ (ì¦ê°• ë°ì´í„° ì‚¬ìš©)...\n",
      "T5 í•™ìŠµ ë°ì´í„°: 17642ê°œ (ì¦ê°•ëœ ë°ì´í„°)\n",
      "T5 ê²€ì¦ ë°ì´í„°: 4411ê°œ (ì¦ê°•ëœ ë°ì´í„°)\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "print(\"ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "train_df = pd.read_csv('./train.csv')\n",
    "print(f\"ì›ë³¸ í•™ìŠµ ë°ì´í„°: {len(train_df)}ê°œ\")\n",
    "\n",
    "# ë°ì´í„° ì¦ê°•\n",
    "print(\"\\në°ì´í„° ì¦ê°• ì¤‘...\")\n",
    "augmented_data = augment_roberta_data(train_df, perm_to_label, multiplier=3)\n",
    "\n",
    "# í•™ìŠµ/ê²€ì¦ ë¶„í• \n",
    "print(\"\\ní•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• ...\")\n",
    "train_data, valid_data = train_test_split(\n",
    "    augmented_data, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=[item[\"label\"] for item in augmented_data]\n",
    ")\n",
    "\n",
    "print(f\"í•™ìŠµ ë°ì´í„°: {len(train_data)}ê°œ\")\n",
    "print(f\"ê²€ì¦ ë°ì´í„°: {len(valid_data)}ê°œ\")\n",
    "\n",
    "# T5ìš© ë°ì´í„° ì¤€ë¹„ (ì¦ê°• ë°ì´í„° ì‚¬ìš©)\n",
    "print(\"\\nT5ìš© ë°ì´í„° ì¤€ë¹„ (ì¦ê°• ë°ì´í„° ì‚¬ìš©)...\")\n",
    "\n",
    "def convert_to_t5_format(augmented_data):\n",
    "    \"\"\"ì¦ê°•ëœ ë°ì´í„°ë¥¼ T5 í˜•ì‹ìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "    t5_data = []\n",
    "    for item in augmented_data:\n",
    "        sentences = item['original_sentences']\n",
    "        answer = item['answer']\n",
    "        \n",
    "        input_text = \"ë¬¸ì¥ì„ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì„¸ìš”: \" + \" </s> \".join(sentences)\n",
    "        target_text = \" \".join(map(str, answer))\n",
    "        \n",
    "        t5_data.append({\n",
    "            \"input\": input_text,\n",
    "            \"target\": target_text\n",
    "        })\n",
    "    return t5_data\n",
    "\n",
    "# ì¦ê°•ëœ train_data, valid_dataë¥¼ T5 í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "t5_train_data = convert_to_t5_format(train_data)\n",
    "t5_valid_data = convert_to_t5_format(valid_data)\n",
    "\n",
    "t5_train_dataset = Dataset.from_pandas(pd.DataFrame(t5_train_data))\n",
    "t5_valid_dataset = Dataset.from_pandas(pd.DataFrame(t5_valid_data))\n",
    "\n",
    "print(f\"T5 í•™ìŠµ ë°ì´í„°: {len(t5_train_data)}ê°œ (ì¦ê°•ëœ ë°ì´í„°)\")\n",
    "print(f\"T5 ê²€ì¦ ë°ì´í„°: {len(t5_valid_data)}ê°œ (ì¦ê°•ëœ ë°ì´í„°)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b05d8dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… T5 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n",
      "T5 ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§• ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17642/17642 [00:10<00:00, 1715.35 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4411/4411 [00:02<00:00, 1715.26 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 í† í¬ë‚˜ì´ì§• ì™„ë£Œ!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"T5 ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©...\")\n",
    "\n",
    "# T5 í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë”©\n",
    "t5_model_name = \"t5-small\"\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name, cache_dir='C:/huggingface_cache')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name, cache_dir='C:/huggingface_cache')\n",
    "t5_model.to(device)\n",
    "\n",
    "print(\"âœ… T5 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "\n",
    "# T5 í† í¬ë‚˜ì´ì§• í•¨ìˆ˜\n",
    "def tokenize_t5(example):\n",
    "    model_inputs = t5_tokenizer(example[\"input\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = t5_tokenizer(example[\"target\"], max_length=16, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# T5 ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§•\n",
    "print(\"T5 ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§• ì¤‘...\")\n",
    "t5_tokenized_train = t5_train_dataset.map(tokenize_t5, batched=True)\n",
    "t5_tokenized_valid = t5_valid_dataset.map(tokenize_t5, batched=True)\n",
    "print(\"T5 í† í¬ë‚˜ì´ì§• ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25f9c966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 ëª¨ë¸ í•™ìŠµ ì‹œì‘...\n",
      "T5 í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7600' max='16545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7600/16545 1:26:32 < 1:41:52, 1.46 it/s, Epoch 6/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.200400</td>\n",
       "      <td>0.183419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.198700</td>\n",
       "      <td>0.185852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.195400</td>\n",
       "      <td>0.180419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.194100</td>\n",
       "      <td>0.174857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.187600</td>\n",
       "      <td>0.171759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.180800</td>\n",
       "      <td>0.170843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.181800</td>\n",
       "      <td>0.170376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.179600</td>\n",
       "      <td>0.170007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.179800</td>\n",
       "      <td>0.169599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.175500</td>\n",
       "      <td>0.166112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.174900</td>\n",
       "      <td>0.163657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.173400</td>\n",
       "      <td>0.160670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.171200</td>\n",
       "      <td>0.160151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.172500</td>\n",
       "      <td>0.156963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.155784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.164200</td>\n",
       "      <td>0.158945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.153827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.161800</td>\n",
       "      <td>0.155321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>0.153812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.164800</td>\n",
       "      <td>0.153249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.158300</td>\n",
       "      <td>0.152927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.152994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.154555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.154700</td>\n",
       "      <td>0.151874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.160900</td>\n",
       "      <td>0.151269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.154900</td>\n",
       "      <td>0.153947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.158300</td>\n",
       "      <td>0.153994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.154700</td>\n",
       "      <td>0.150499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.156900</td>\n",
       "      <td>0.149693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.155900</td>\n",
       "      <td>0.149324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.156900</td>\n",
       "      <td>0.147301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.154600</td>\n",
       "      <td>0.146427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.156000</td>\n",
       "      <td>0.148685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.154100</td>\n",
       "      <td>0.146496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.158000</td>\n",
       "      <td>0.144943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.154400</td>\n",
       "      <td>0.148164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.147644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.152600</td>\n",
       "      <td>0.146666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7600, training_loss=0.16754526602594477, metrics={'train_runtime': 5206.008, 'train_samples_per_second': 50.832, 'train_steps_per_second': 3.178, 'total_flos': 1.6451066652524544e+16, 'train_loss': 0.16754526602594477, 'epoch': 6.890702947845805})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"T5 ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "\n",
    "# EarlyStoppingCallback ì¶”ê°€\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# T5 í•™ìŠµ ì„¤ì • (RTX 2070 Super 8GB ìµœì í™”)\n",
    "t5_training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_results\",\n",
    "    learning_rate=3e-5,  # ì¡°ê¸ˆ ë†’ì—¬ì„œ ë¹ ë¥¸ ìˆ˜ë ´\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ìµœì í™” ë°°ì¹˜ ì„¤ì •\n",
    "    per_device_train_batch_size=4,  # 16 â†’ 4ë¡œ ëŒ€í­ ì¶•ì†Œ\n",
    "    per_device_eval_batch_size=8,   # 32 â†’ 8ë¡œ ì¶•ì†Œ\n",
    "    gradient_accumulation_steps=4,  # ì‹¤ì œ ë°°ì¹˜ í¬ê¸°: 4*4=16\n",
    "    \n",
    "    num_train_epochs=15,  # 20 â†’ 15ë¡œ ë‹¨ì¶•\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •\n",
    "    fp16=True,  # ë©”ëª¨ë¦¬ 50% ì ˆì•½\n",
    "    gradient_checkpointing=True,  # ë©”ëª¨ë¦¬ ì ˆì•½ (ì†ë„ 20% ê°ì†Œ)\n",
    "    dataloader_pin_memory=False,  # ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "    dataloader_num_workers=2,     # 4 â†’ 2ë¡œ ì¶•ì†Œ\n",
    "    \n",
    "    # í‰ê°€ ë° ì €ì¥ (ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•´ ë” ìì£¼)\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,  # 100 â†’ 200ìœ¼ë¡œ ëŠ˜ë ¤ì„œ ë©”ëª¨ë¦¬ ì••ë°• ì™„í™”\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,  # 3 â†’ 2ë¡œ ì¶•ì†Œ (ë””ìŠ¤í¬ ê³µê°„ ì ˆì•½)\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # ë¡œê¹…\n",
    "    logging_steps=100,\n",
    "    report_to=None,\n",
    "    \n",
    "    # ì¶”ê°€ ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "    remove_unused_columns=True,\n",
    "    dataloader_drop_last=True,\n",
    ")\n",
    "\n",
    "# T5 Trainer ì •ì˜ (EarlyStoppingCallback ì¶”ê°€)\n",
    "t5_trainer = Trainer(\n",
    "    model=t5_model,\n",
    "    args=t5_training_args,\n",
    "    train_dataset=t5_tokenized_train,\n",
    "    eval_dataset=t5_tokenized_valid,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # 3ë²ˆ ì—°ì† ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨\n",
    ")\n",
    "\n",
    "print(\"T5 í•™ìŠµ ì‹œì‘...\")\n",
    "t5_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8815f19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 ëª¨ë¸ ì €ì¥ ì¤‘...\n",
      "âœ… T5 í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# T5 ëª¨ë¸ ì €ì¥\n",
    "print(\"T5 ëª¨ë¸ ì €ì¥ ì¤‘...\")\n",
    "t5_trainer.save_model(\"./t5_results/final_model\")\n",
    "t5_tokenizer.save_pretrained(\"./t5_results/final_model\")\n",
    "print(\"âœ… T5 í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53c262f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©...\n",
      "í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ: BertTokenizerFast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RoBERTa ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n",
      "RoBERTa ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§• ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17642/17642 [00:02<00:00, 8513.26 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4411/4411 [00:00<00:00, 8505.29 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa í† í¬ë‚˜ì´ì§• ì™„ë£Œ!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"RoBERTa ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©...\")\n",
    "\n",
    "# RoBERTa ëª¨ë¸ ì„¤ì •\n",
    "roberta_model_name = \"klue/roberta-base\"\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_name, cache_dir='C:/huggingface_cache')\n",
    "print(f\"í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ: {type(roberta_tokenizer).__name__}\")\n",
    "\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    roberta_model_name,\n",
    "    num_labels=24,  # 24ê°€ì§€ ìˆœì—´\n",
    "    cache_dir='C:/huggingface_cache'\n",
    ")\n",
    "roberta_model.to(device)\n",
    "\n",
    "print(\"âœ… RoBERTa ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "\n",
    "# RoBERTa ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "train_df_processed = pd.DataFrame(train_data)\n",
    "valid_df_processed = pd.DataFrame(valid_data)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df_processed)\n",
    "valid_dataset = Dataset.from_pandas(valid_df_processed)\n",
    "\n",
    "# RoBERTa í† í¬ë‚˜ì´ì§• í•¨ìˆ˜\n",
    "def tokenize_roberta(examples):\n",
    "    return roberta_tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# RoBERTa ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§•\n",
    "print(\"RoBERTa ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§• ì¤‘...\")\n",
    "roberta_tokenized_train = train_dataset.map(tokenize_roberta, batched=True)\n",
    "roberta_tokenized_valid = valid_dataset.map(tokenize_roberta, batched=True)\n",
    "\n",
    "# ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°\n",
    "roberta_tokenized_train = roberta_tokenized_train.remove_columns([\"text\", \"original_sentences\", \"answer\"])\n",
    "roberta_tokenized_valid = roberta_tokenized_valid.remove_columns([\"text\", \"original_sentences\", \"answer\"])\n",
    "\n",
    "print(\"RoBERTa í† í¬ë‚˜ì´ì§• ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11914a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa ëª¨ë¸ í•™ìŠµ ì‹œì‘...\n",
      "RoBERTa í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4140' max='4140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4140/4140 46:42, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.580900</td>\n",
       "      <td>2.467889</td>\n",
       "      <td>0.122719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.822300</td>\n",
       "      <td>1.577760</td>\n",
       "      <td>0.438412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.826100</td>\n",
       "      <td>0.842864</td>\n",
       "      <td>0.667427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.588100</td>\n",
       "      <td>0.545402</td>\n",
       "      <td>0.818203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.388300</td>\n",
       "      <td>0.419764</td>\n",
       "      <td>0.868385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.221400</td>\n",
       "      <td>0.336767</td>\n",
       "      <td>0.907162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.175700</td>\n",
       "      <td>0.303742</td>\n",
       "      <td>0.919024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.152400</td>\n",
       "      <td>0.263528</td>\n",
       "      <td>0.929516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.103400</td>\n",
       "      <td>0.216563</td>\n",
       "      <td>0.943203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.219737</td>\n",
       "      <td>0.950730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.062800</td>\n",
       "      <td>0.168223</td>\n",
       "      <td>0.962819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.164478</td>\n",
       "      <td>0.966925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>0.167992</td>\n",
       "      <td>0.968066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>0.177671</td>\n",
       "      <td>0.964416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>0.154645</td>\n",
       "      <td>0.971943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.145898</td>\n",
       "      <td>0.974909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.150761</td>\n",
       "      <td>0.972400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.145341</td>\n",
       "      <td>0.974224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.138861</td>\n",
       "      <td>0.976049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.139987</td>\n",
       "      <td>0.975593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4140, training_loss=0.405164846583553, metrics={'train_runtime': 2824.0126, 'train_samples_per_second': 93.707, 'train_steps_per_second': 1.466, 'total_flos': 1.9441645636299264e+16, 'train_loss': 0.405164846583553, 'epoch': 15.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"RoBERTa ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "\n",
    "# EarlyStoppingCallback import (ì´ë¯¸ ìœ„ì—ì„œ í–ˆì§€ë§Œ ì•ˆì „í•˜ê²Œ)\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# í‰ê°€ ë©”íŠ¸ë¦­ ì •ì˜\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# RoBERTa í•™ìŠµ ì„¤ì • (RTX 2070 Super 8GB ìµœì í™”)\n",
    "roberta_training_args = TrainingArguments(\n",
    "    output_dir=\"./roberta_results\",\n",
    "    learning_rate=3e-5,  # ì¡°ê¸ˆ ë†’ì—¬ì„œ ë¹ ë¥¸ ìˆ˜ë ´\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ìµœì í™” ë°°ì¹˜ ì„¤ì •\n",
    "    per_device_train_batch_size=16,  # 64 â†’ 16ë¡œ ì¶•ì†Œ\n",
    "    per_device_eval_batch_size=32,   # 128 â†’ 32ë¡œ ì¶•ì†Œ  \n",
    "    gradient_accumulation_steps=4,   # 2 â†’ 4ë¡œ ì¦ê°€, ì‹¤ì œ ë°°ì¹˜: 16*4=64\n",
    "    \n",
    "    num_train_epochs=15,  # 20 â†’ 15ë¡œ ë‹¨ì¶•\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,  # ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "    dataloader_pin_memory=False,  # ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "    dataloader_num_workers=2,     # 6 â†’ 2ë¡œ ì¶•ì†Œ\n",
    "    \n",
    "    # í‰ê°€ ë° ì €ì¥ (ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•´ ë” ìì£¼)\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,  # 100 â†’ 200ìœ¼ë¡œ ëŠ˜ë ¤ì„œ ë©”ëª¨ë¦¬ ì••ë°• ì™„í™”\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,  # 3 â†’ 2ë¡œ ì¶•ì†Œ\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_accuracy\",  # ì •í™•ë„ ê¸°ì¤€\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # í•™ìŠµ ìµœì í™”\n",
    "    warmup_steps=200,  # 150 â†’ 200\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # ë¡œê¹…\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,  # 20 â†’ 50\n",
    "    report_to=None,\n",
    "    \n",
    "    # ì¶”ê°€ ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "    remove_unused_columns=True,\n",
    "    dataloader_drop_last=True,\n",
    ")\n",
    "\n",
    "# ë°ì´í„° ì½œë ˆì´í„°\n",
    "data_collator = DataCollatorWithPadding(tokenizer=roberta_tokenizer)\n",
    "\n",
    "# RoBERTa Trainer ì„¤ì • (EarlyStoppingCallback ì¶”ê°€)\n",
    "roberta_trainer = Trainer(\n",
    "    model=roberta_model,\n",
    "    args=roberta_training_args,\n",
    "    train_dataset=roberta_tokenized_train,\n",
    "    eval_dataset=roberta_tokenized_valid,\n",
    "    tokenizer=roberta_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # 3ë²ˆ ì—°ì† ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨\n",
    ")\n",
    "\n",
    "print(\"RoBERTa í•™ìŠµ ì‹œì‘...\")\n",
    "roberta_trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27968784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa ëª¨ë¸ ì €ì¥ ì¤‘...\n",
      "âœ… RoBERTa í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# RoBERTa ëª¨ë¸ ì €ì¥\n",
    "print(\"RoBERTa ëª¨ë¸ ì €ì¥ ì¤‘...\")\n",
    "roberta_trainer.save_model(\"./roberta_results/final_model\")\n",
    "roberta_tokenizer.save_pretrained(\"./roberta_results/final_model\")\n",
    "print(\"âœ… RoBERTa í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55fcb696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµëœ ëª¨ë¸ë“¤ ë¡œë“œ ì¤‘...\n",
      "í•™ìŠµëœ T5 ëª¨ë¸ ë¡œë”©...\n",
      "âœ… í•™ìŠµëœ T5 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n",
      "í•™ìŠµëœ RoBERTa ëª¨ë¸ ë¡œë”©...\n",
      "âœ… í•™ìŠµëœ RoBERTa ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n",
      "\n",
      "ğŸ‰ ëª¨ë“  ëª¨ë¸ í•™ìŠµ ë° ë¡œë“œ ì™„ë£Œ!\n",
      "GPU ë©”ëª¨ë¦¬: 3.05GB ì‚¬ìš© / 8.00GB ì „ì²´ (38.1% ì‚¬ìš©)\n"
     ]
    }
   ],
   "source": [
    "print(\"í•™ìŠµëœ ëª¨ë¸ë“¤ ë¡œë“œ ì¤‘...\")\n",
    "\n",
    "# í•™ìŠµëœ T5 ëª¨ë¸ ë¡œë“œ\n",
    "print(\"í•™ìŠµëœ T5 ëª¨ë¸ ë¡œë”©...\")\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"./t5_results/final_model\")\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"./t5_results/final_model\")\n",
    "t5_model.to(device)\n",
    "t5_model.eval()\n",
    "print(\"âœ… í•™ìŠµëœ T5 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "\n",
    "# í•™ìŠµëœ RoBERTa ëª¨ë¸ ë¡œë“œ\n",
    "print(\"í•™ìŠµëœ RoBERTa ëª¨ë¸ ë¡œë”©...\")\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"./roberta_results/final_model\")\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(\"./roberta_results/final_model\")\n",
    "roberta_model.to(device)\n",
    "roberta_model.eval()\n",
    "print(\"âœ… í•™ìŠµëœ RoBERTa ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "\n",
    "print(\"\\nğŸ‰ ëª¨ë“  ëª¨ë¸ í•™ìŠµ ë° ë¡œë“œ ì™„ë£Œ!\")\n",
    "\n",
    "# GPU ë©”ëª¨ë¦¬ ì²´í¬\n",
    "def print_gpu_utilization():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        usage_percent = (allocated / total) * 100\n",
    "        print(f\"GPU ë©”ëª¨ë¦¬: {allocated:.2f}GB ì‚¬ìš© / {total:.2f}GB ì „ì²´ ({usage_percent:.1f}% ì‚¬ìš©)\")\n",
    "    else:\n",
    "        print(\"CUDA ì‚¬ìš© ë¶ˆê°€\")\n",
    "\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f386838d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•™ìƒë¸” ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "def predict_t5(sentences, model, tokenizer, device):\n",
    "    \"\"\"T5 ëª¨ë¸ë¡œ ì˜ˆì¸¡\"\"\"\n",
    "    input_text = \"ë¬¸ì¥ì„ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì„¸ìš”: \" + \" </s> \".join(sentences)\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_length=16,\n",
    "            do_sample=True,      \n",
    "            temperature=0.2,     \n",
    "            top_p=0.9,            \n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    try:\n",
    "        order = list(map(int, decoded.strip().split()))\n",
    "        if len(order) == 4 and set(order) == {0, 1, 2, 3}:\n",
    "            return order, 1.0\n",
    "        else:\n",
    "            return [0, 1, 2, 3], 0.1\n",
    "    except:\n",
    "        return [0, 1, 2, 3], 0.1\n",
    "\n",
    "def predict_roberta(sentences, model, tokenizer, label_to_perm, device):\n",
    "    \"\"\"RoBERTa ëª¨ë¸ë¡œ ì˜ˆì¸¡\"\"\"\n",
    "    text = \" [SEP] \".join(sentences)\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_label = torch.argmax(predictions, dim=-1).item()\n",
    "        confidence = predictions[0][predicted_label].item()\n",
    "    \n",
    "    predicted_order = list(label_to_perm[predicted_label])\n",
    "    return predicted_order, confidence\n",
    "\n",
    "def ensemble_predict_confidence_based(sentences, t5_model, t5_tokenizer, roberta_model, roberta_tokenizer, label_to_perm, device, confidence_threshold=0.7):\n",
    "    \"\"\"ì‹ ë¢°ë„ ê¸°ë°˜ ì•™ìƒë¸” ì˜ˆì¸¡\"\"\"\n",
    "    # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡\n",
    "    t5_pred, t5_conf = predict_t5(sentences, t5_model, t5_tokenizer, device)\n",
    "    roberta_pred, roberta_conf = predict_roberta(sentences, roberta_model, roberta_tokenizer, label_to_perm, device)\n",
    "    \n",
    "    # RoBERTa ì‹ ë¢°ë„ê°€ ì„ê³„ê°’ë³´ë‹¤ ë†’ìœ¼ë©´ RoBERTa ì„ íƒ\n",
    "    if roberta_conf >= confidence_threshold:\n",
    "        return roberta_pred, roberta_conf, \"roberta\"\n",
    "    else:\n",
    "        return t5_pred, t5_conf, \"t5\"\n",
    "\n",
    "print(\"ì•™ìƒë¸” ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02b3226e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€ ì¤‘... (ì‹ ë¢°ë„ ì„ê³„ê°’: 0.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í‰ê°€ ì§„í–‰: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4411/4411 [05:49<00:00, 12.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ì•™ìƒë¸” í‰ê°€ ê²°ê³¼ ===\n",
      "ì •í™•ë„: 0.9739 (97.39%)\n",
      "ì •ë‹µ ê°œìˆ˜: 4296/4411\n",
      "í‰ê·  ì‹ ë¢°ë„: 0.9967\n",
      "ëª¨ë¸ ì„ íƒ í†µê³„: {'roberta': 4391, 't5': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_ensemble_performance(valid_data, confidence_threshold=0.7):\n",
    "    \"\"\"ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€\"\"\"\n",
    "    correct = 0\n",
    "    total = len(valid_data)\n",
    "    \n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    model_selections = []\n",
    "    \n",
    "    print(f\"ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€ ì¤‘... (ì‹ ë¢°ë„ ì„ê³„ê°’: {confidence_threshold})\")\n",
    "    \n",
    "    for example in tqdm(valid_data, desc=\"í‰ê°€ ì§„í–‰\"):\n",
    "        sentences = example['original_sentences']\n",
    "        true_order = list(example['answer'])\n",
    "        \n",
    "        pred_order, confidence, selected_model = ensemble_predict_confidence_based(\n",
    "            sentences, t5_model, t5_tokenizer, roberta_model, roberta_tokenizer, \n",
    "            label_to_perm, device, confidence_threshold\n",
    "        )\n",
    "        \n",
    "        predictions.append(pred_order)\n",
    "        confidences.append(confidence)\n",
    "        model_selections.append(selected_model)\n",
    "        \n",
    "        if pred_order == true_order:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    avg_confidence = np.mean(confidences)\n",
    "    \n",
    "    # ëª¨ë¸ ì„ íƒ í†µê³„\n",
    "    model_stats = Counter(model_selections)\n",
    "    \n",
    "    print(f\"\\n=== ì•™ìƒë¸” í‰ê°€ ê²°ê³¼ ===\")\n",
    "    print(f\"ì •í™•ë„: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"ì •ë‹µ ê°œìˆ˜: {correct}/{total}\")\n",
    "    print(f\"í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.4f}\")\n",
    "    print(f\"ëª¨ë¸ ì„ íƒ í†µê³„: {dict(model_stats)}\")\n",
    "    \n",
    "    return accuracy, predictions, confidences, model_selections\n",
    "\n",
    "# ì„±ëŠ¥ í‰ê°€ ì‹¤í–‰\n",
    "accuracy, val_predictions, val_confidences, val_selections = evaluate_ensemble_performance(valid_data, confidence_threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e96679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„°: 1780ê°œ\n",
      "ì‚¬ìš©í•  ì‹ ë¢°ë„ ì„ê³„ê°’: 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì•™ìƒë¸” ì˜ˆì¸¡ ì§„í–‰: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1780/1780 [02:23<00:00, 12.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ì™„ë£Œ ===\n",
      "í‰ê·  ì˜ˆì¸¡ ì‹ ë¢°ë„: 0.9917\n",
      "ëª¨ë¸ ì„ íƒ í†µê³„: {'roberta': 1755, 't5': 25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "test_df = pd.read_csv(\"./test.csv\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ê°œ\")\n",
    "\n",
    "predictions = []\n",
    "confidences = []\n",
    "model_selections = []\n",
    "\n",
    "# ì‹ ë¢°ë„ ì„ê³„ê°’ ì„¤ì •\n",
    "optimal_threshold = 0.8\n",
    "print(f\"ì‚¬ìš©í•  ì‹ ë¢°ë„ ì„ê³„ê°’: {optimal_threshold}\")\n",
    "\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"ì•™ìƒë¸” ì˜ˆì¸¡ ì§„í–‰\"):\n",
    "    sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "    \n",
    "    predicted_order, confidence, selected_model = ensemble_predict_confidence_based(\n",
    "        sentences, t5_model, t5_tokenizer, roberta_model, roberta_tokenizer, \n",
    "        label_to_perm, device, optimal_threshold\n",
    "    )\n",
    "    \n",
    "    predictions.append(predicted_order)\n",
    "    confidences.append(confidence)\n",
    "    model_selections.append(selected_model)\n",
    "\n",
    "# ì˜ˆì¸¡ í†µê³„\n",
    "avg_confidence = np.mean(confidences)\n",
    "model_stats = Counter(model_selections)\n",
    "\n",
    "print(f\"\\n=== í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ì™„ë£Œ ===\")\n",
    "print(f\"í‰ê·  ì˜ˆì¸¡ ì‹ ë¢°ë„: {avg_confidence:.4f}\")\n",
    "print(f\"ëª¨ë¸ ì„ íƒ í†µê³„: {dict(model_stats)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d00ba59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission íŒŒì¼ ìƒì„± ì¤‘...\n",
      "ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: ensemble_t5_roberta_finetuned.csv\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ T5 + RoBERTa Fine-tuned ì•™ìƒë¸” ì™„ë£Œ!\n",
      "============================================================\n",
      "ğŸ“Š ê²€ì¦ ì •í™•ë„: 0.9739 (97.39%)\n",
      "ğŸ”® í‰ê·  ì˜ˆì¸¡ ì‹ ë¢°ë„: 99.17%\n",
      "ğŸšï¸ ì‹ ë¢°ë„ ì„ê³„ê°’: 0.7\n",
      "ğŸ¤– í…ŒìŠ¤íŠ¸ì…‹ ëª¨ë¸ ì„ íƒ: {'roberta': 1755, 't5': 25}\n",
      "ğŸ“ ì œì¶œ íŒŒì¼: ensemble_t5_roberta_finetuned.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Submission íŒŒì¼ ìƒì„± ì¤‘...\")\n",
    "\n",
    "# ìƒ˜í”Œ ì œì¶œ íŒŒì¼ ë¡œë“œ\n",
    "sample_submission = pd.read_csv(\"./sample_submission.csv\")\n",
    "\n",
    "# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì œì¶œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "for i in range(4):\n",
    "    sample_submission[f\"answer_{i}\"] = [pred[i] for pred in predictions]\n",
    "\n",
    "# íŒŒì¼ ì €ì¥\n",
    "submission_filename = \"ensemble_t5_roberta_finetuned.csv\"\n",
    "sample_submission.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {submission_filename}\")\n",
    "\n",
    "# ìµœì¢… ê²°ê³¼ ìš”ì•½\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ¯ T5 + RoBERTa Fine-tuned ì•™ìƒë¸” ì™„ë£Œ!\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"ğŸ“Š ê²€ì¦ ì •í™•ë„: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"ğŸ”® í‰ê·  ì˜ˆì¸¡ ì‹ ë¢°ë„: {avg_confidence*100:.2f}%\")\n",
    "print(f\"ğŸšï¸ ì‹ ë¢°ë„ ì„ê³„ê°’: {optimal_threshold}\")\n",
    "print(f\"ğŸ¤– í…ŒìŠ¤íŠ¸ì…‹ ëª¨ë¸ ì„ íƒ: {dict(model_stats)}\")\n",
    "print(f\"ğŸ“ ì œì¶œ íŒŒì¼: {submission_filename}\")\n",
    "print(f\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb574eec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daycon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
