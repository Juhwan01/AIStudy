{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "703e1e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import lightgbm as lgb\n",
    "import safetensors\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from itertools import permutations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74934889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 24개의 순열 클래스 생성\n",
      "예시 매핑:\n",
      "  라벨 0: (0, 1, 2, 3)\n",
      "  라벨 1: (0, 1, 3, 2)\n",
      "  라벨 2: (0, 2, 1, 3)\n",
      "  라벨 3: (0, 2, 3, 1)\n",
      "  라벨 4: (0, 3, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "def create_label_mappings():\n",
    "    \"\"\"24가지 순열에 대한 라벨 매핑 생성\"\"\"\n",
    "    all_permutations = list(permutations([0, 1, 2, 3]))\n",
    "    perm_to_label = {perm: idx for idx, perm in enumerate(all_permutations)}\n",
    "    label_to_perm = {idx: perm for idx, perm in enumerate(all_permutations)}\n",
    "    \n",
    "    print(f\"총 {len(all_permutations)}개의 순열 클래스 생성\")\n",
    "    print(\"예시 매핑:\")\n",
    "    for i in range(5):\n",
    "        print(f\"  라벨 {i}: {all_permutations[i]}\")\n",
    "    \n",
    "    return perm_to_label, label_to_perm\n",
    "\n",
    "# 순열 매핑 생성\n",
    "perm_to_label, label_to_perm = create_label_mappings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bef94fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_roberta_data(train_df, perm_to_label):\n",
    "    \"\"\"RoBERTa용 데이터 준비\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for _, row in train_df.iterrows():\n",
    "        sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "        answer_tuple = tuple([row[f\"answer_{i}\"] for i in range(4)])\n",
    "        text = \" [SEP] \".join(sentences)\n",
    "        label = perm_to_label[answer_tuple]\n",
    "        \n",
    "        processed_data.append({\n",
    "            \"text\": text,\n",
    "            \"label\": label,\n",
    "            \"original_sentences\": sentences,\n",
    "            \"answer\": answer_tuple\n",
    "        })\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "def augment_roberta_data_advanced(train_df, perm_to_label, multiplier=4):\n",
    "    \"\"\"고급 데이터 증강\"\"\"\n",
    "    augmented_data = []\n",
    "    \n",
    "    # 원본 데이터\n",
    "    original_data = prepare_roberta_data(train_df, perm_to_label)\n",
    "    augmented_data.extend(original_data)\n",
    "    \n",
    "    # 다양한 증강 방법\n",
    "    for aug_round in range(multiplier - 1):\n",
    "        for _, row in train_df.iterrows():\n",
    "            sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "            original_answer = [row[f\"answer_{i}\"] for i in range(4)]\n",
    "            \n",
    "            if aug_round == 0:\n",
    "                # 랜덤 셔플\n",
    "                indices = list(range(4))\n",
    "                np.random.shuffle(indices)\n",
    "            elif aug_round == 1:\n",
    "                # 순환 이동\n",
    "                shift = np.random.randint(1, 4)\n",
    "                indices = [(i + shift) % 4 for i in range(4)]\n",
    "            else:\n",
    "                # 부분 교환\n",
    "                indices = list(range(4))\n",
    "                i, j = np.random.choice(4, 2, replace=False)\n",
    "                indices[i], indices[j] = indices[j], indices[i]\n",
    "            \n",
    "            shuffled_sentences = [sentences[i] for i in indices]\n",
    "            new_answer = tuple([indices.index(original_answer[i]) for i in range(4)])\n",
    "            \n",
    "            text = \" [SEP] \".join(shuffled_sentences)\n",
    "            label = perm_to_label[new_answer]\n",
    "            \n",
    "            augmented_data.append({\n",
    "                \"text\": text,\n",
    "                \"label\": label,\n",
    "                \"original_sentences\": shuffled_sentences,\n",
    "                \"answer\": new_answer\n",
    "            })\n",
    "    \n",
    "    print(f\"고급 데이터 증강 완료: {len(original_data)} → {len(augmented_data)}\")\n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b89decf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로드 중...\n",
      "원본 학습 데이터: 7351개\n",
      "\n",
      "고급 데이터 증강 중...\n",
      "고급 데이터 증강 완료: 7351 → 29404\n",
      "\n",
      "학습/검증 데이터 분할...\n",
      "학습 데이터: 23523개\n",
      "검증 데이터: 5881개\n"
     ]
    }
   ],
   "source": [
    "print(\"데이터 로드 중...\")\n",
    "train_df = pd.read_csv('./train.csv')\n",
    "print(f\"원본 학습 데이터: {len(train_df)}개\")\n",
    "\n",
    "print(\"\\n고급 데이터 증강 중...\")\n",
    "augmented_data = augment_roberta_data_advanced(train_df, perm_to_label, multiplier=4)\n",
    "\n",
    "print(\"\\n학습/검증 데이터 분할...\")\n",
    "train_data, valid_data = train_test_split(\n",
    "    augmented_data, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=[item[\"label\"] for item in augmented_data]\n",
    ")\n",
    "\n",
    "print(f\"학습 데이터: {len(train_data)}개\")\n",
    "print(f\"검증 데이터: {len(valid_data)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90071e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 셀 5: Stacking 앙상블 클래스 정의 (완전 수정됨)\n",
    "# =============================================================================\n",
    "class AdvancedStackingEnsemble:\n",
    "    \"\"\"최고 성능 Stacking 앙상블\"\"\"\n",
    "    \n",
    "    def __init__(self, n_folds=5, random_state=42):\n",
    "        self.n_folds = n_folds\n",
    "        self.random_state = random_state\n",
    "        self.base_models = []\n",
    "        self.meta_model = None\n",
    "        self.base_model_configs = [\n",
    "            {\n",
    "                'name': 'bert_small', \n",
    "                'model_name': 'klue/bert-base',  # BERT 아키텍처 (첫 번째)\n",
    "                'learning_rate': 1.5e-5,\n",
    "                'epochs': 25,\n",
    "                'batch_size': 48,\n",
    "                'warmup_steps': 200\n",
    "            },\n",
    "            {\n",
    "                'name': 'roberta_small',\n",
    "                'model_name': 'klue/roberta-small',\n",
    "                'learning_rate': 2e-5,\n",
    "                'epochs': 20,\n",
    "                'batch_size': 64,\n",
    "                'warmup_steps': 150\n",
    "            },\n",
    "            {\n",
    "                'name': 'electra_small',\n",
    "                'model_name': 'monologg/koelectra-small-v3-discriminator',  # ELECTRA 아키텍처 (마지막)\n",
    "                'learning_rate': 3e-5,\n",
    "                'epochs': 15,\n",
    "                'batch_size': 80,\n",
    "                'warmup_steps': 100\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "    def train_base_model(self, config, train_dataset, valid_dataset, tokenizer, device):\n",
    "        \"\"\"개별 베이스 모델 학습\"\"\"\n",
    "        print(f\"\\n🔥 {config['name']} 모델 학습 시작...\")\n",
    "        \n",
    "        # 모델 생성\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            config['model_name'],\n",
    "            num_labels=24,\n",
    "            cache_dir='C:/huggingface_cache'\n",
    "        )\n",
    "        model.to(device)\n",
    "        \n",
    "        # 학습 설정 (조기 종료 포함)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./results_{config['name']}\",\n",
    "            learning_rate=config['learning_rate'],\n",
    "            per_device_train_batch_size=config['batch_size'],\n",
    "            per_device_eval_batch_size=128,\n",
    "            gradient_accumulation_steps=2,\n",
    "            num_train_epochs=config['epochs'],\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=150,\n",
    "            save_strategy=\"steps\", \n",
    "            save_steps=150,\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            greater_is_better=True,\n",
    "            fp16=True,\n",
    "            dataloader_pin_memory=True,\n",
    "            dataloader_num_workers=6,\n",
    "            warmup_steps=config['warmup_steps'],\n",
    "            weight_decay=0.01,\n",
    "            logging_steps=20,\n",
    "            report_to=None,\n",
    "        )\n",
    "        \n",
    "        # 트레이너 생성 (조기 종료 콜백 포함)\n",
    "        from transformers import EarlyStoppingCallback\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=valid_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "            compute_metrics=lambda eval_pred: {\n",
    "                \"accuracy\": accuracy_score(eval_pred.label_ids, np.argmax(eval_pred.predictions, axis=1))\n",
    "            },\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "        )\n",
    "        \n",
    "        # 학습 실행\n",
    "        trainer.train()\n",
    "        \n",
    "        # 모델 저장\n",
    "        model_save_path = f\"./results_{config['name']}/final\"\n",
    "        trainer.save_model(model_save_path)\n",
    "        \n",
    "        print(f\"✅ {config['name']} 모델 학습 완료!\")\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'trainer': trainer,\n",
    "            'config': config,\n",
    "            'save_path': model_save_path\n",
    "        }\n",
    "    \n",
    "    def generate_meta_features(self, models, data, device):\n",
    "        \"\"\"메타 특성 생성 (K-Fold 교차 검증)\"\"\"\n",
    "        print(\"\\n🧠 메타 특성 생성 중...\")\n",
    "        \n",
    "        # 데이터를 DataFrame으로 변환\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # K-Fold 설정\n",
    "        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        # 메타 특성 저장할 배열\n",
    "        meta_features = np.zeros((len(data), len(models) * 24))  # 각 모델당 24개 클래스 확률\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
    "            print(f\"  Fold {fold + 1}/{self.n_folds} 처리 중...\")\n",
    "            \n",
    "            # 검증 데이터 추출\n",
    "            val_data = df.iloc[val_idx]\n",
    "            \n",
    "            # 각 베이스 모델로 예측\n",
    "            for model_idx, model_info in enumerate(models):\n",
    "                model = model_info['model']\n",
    "                model.eval()\n",
    "                \n",
    "                # 해당 모델의 토크나이저 로드\n",
    "                tokenizer = AutoTokenizer.from_pretrained(\n",
    "                    model_info['config']['model_name'],\n",
    "                    cache_dir='C:/huggingface_cache'\n",
    "                )\n",
    "                \n",
    "                fold_predictions = []\n",
    "                \n",
    "                # 배치 단위로 예측\n",
    "                for _, row in val_data.iterrows():\n",
    "                    text = row['text']\n",
    "                    \n",
    "                    inputs = tokenizer(\n",
    "                        text,\n",
    "                        return_tensors=\"pt\",\n",
    "                        truncation=True,\n",
    "                        padding=True,\n",
    "                        max_length=512\n",
    "                    ).to(device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(**inputs)\n",
    "                        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                        fold_predictions.append(probabilities.cpu().numpy()[0])\n",
    "                \n",
    "                # 메타 특성에 저장\n",
    "                start_col = model_idx * 24\n",
    "                end_col = (model_idx + 1) * 24\n",
    "                meta_features[val_idx, start_col:end_col] = np.array(fold_predictions)\n",
    "        \n",
    "        return meta_features\n",
    "    \n",
    "    def train_meta_model(self, meta_features, labels):\n",
    "        \"\"\"메타 모델 학습\"\"\"\n",
    "        print(\"\\n🎯 메타 모델 학습 중...\")\n",
    "        \n",
    "        # LightGBM을 메타 모델로 사용 (가장 효과적)\n",
    "        self.meta_model = lgb.LGBMClassifier(\n",
    "            objective='multiclass',\n",
    "            num_class=24,\n",
    "            boosting_type='gbdt',\n",
    "            num_leaves=31,\n",
    "            learning_rate=0.05,\n",
    "            feature_fraction=0.9,\n",
    "            bagging_fraction=0.8,\n",
    "            bagging_freq=5,\n",
    "            verbose=0,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # 메타 모델 학습\n",
    "        self.meta_model.fit(meta_features, labels)\n",
    "        \n",
    "        print(\"✅ 메타 모델 학습 완료!\")\n",
    "        \n",
    "        return self.meta_model\n",
    "    \n",
    "    def fit(self, train_data, valid_data, device):\n",
    "        \"\"\"전체 스택킹 앙상블 학습\"\"\"\n",
    "        print(\"🚀 Stacking 앙상블 학습 시작!\")\n",
    "        \n",
    "        # 데이터셋 생성\n",
    "        train_df = pd.DataFrame(train_data)\n",
    "        valid_df = pd.DataFrame(valid_data)\n",
    "        \n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        valid_dataset = Dataset.from_pandas(valid_df)\n",
    "        \n",
    "        # 1단계: 베이스 모델들 학습\n",
    "        print(\"\\n📚 1단계: 베이스 모델들 학습\")\n",
    "        for config in self.base_model_configs:\n",
    "            # 각 모델에 맞는 토크나이저 로드\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                config['model_name'],\n",
    "                cache_dir='C:/huggingface_cache'\n",
    "            )\n",
    "            print(f\"✅ {config['name']} 토크나이저 로드 완료\")\n",
    "            \n",
    "            # 토크나이징 함수\n",
    "            def tokenize_function(examples):\n",
    "                return tokenizer(\n",
    "                    examples[\"text\"],\n",
    "                    truncation=True,\n",
    "                    padding=True,\n",
    "                    max_length=512\n",
    "                )\n",
    "            \n",
    "            # 각 모델별로 데이터셋 토크나이징\n",
    "            current_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "            current_valid_dataset = valid_dataset.map(tokenize_function, batched=True)\n",
    "            \n",
    "            current_train_dataset = current_train_dataset.remove_columns([\"text\", \"original_sentences\", \"answer\"])\n",
    "            current_valid_dataset = current_valid_dataset.remove_columns([\"text\", \"original_sentences\", \"answer\"])\n",
    "            \n",
    "            # 모델 학습\n",
    "            model_info = self.train_base_model(config, current_train_dataset, current_valid_dataset, tokenizer, device)\n",
    "            self.base_models.append(model_info)\n",
    "        \n",
    "        # 2단계: 메타 특성 생성\n",
    "        print(\"\\n🔧 2단계: 메타 특성 생성\")\n",
    "        meta_features = self.generate_meta_features(self.base_models, train_data, device)\n",
    "        labels = [item['label'] for item in train_data]\n",
    "        \n",
    "        # 3단계: 메타 모델 학습\n",
    "        print(\"\\n🎓 3단계: 메타 모델 학습\")\n",
    "        self.train_meta_model(meta_features, labels)\n",
    "        \n",
    "        print(\"\\n🎉 Stacking 앙상블 학습 완료!\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, test_data, device):\n",
    "        \"\"\"스택킹 앙상블 예측\"\"\"\n",
    "        print(\"\\n🔮 Stacking 앙상블 예측 중...\")\n",
    "        \n",
    "        # 테스트 데이터로 베이스 모델 예측\n",
    "        test_meta_features = []\n",
    "        \n",
    "        for model_info in self.base_models:\n",
    "            model = model_info['model']\n",
    "            model.eval()\n",
    "            \n",
    "            # 해당 모델의 토크나이저 로드\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_info['config']['model_name'],\n",
    "                cache_dir='C:/huggingface_cache'\n",
    "            )\n",
    "            \n",
    "            model_predictions = []\n",
    "            \n",
    "            for text in tqdm(test_data, desc=f\"{model_info['config']['name']} 예측\"):\n",
    "                inputs = tokenizer(\n",
    "                    text,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    padding=True,\n",
    "                    max_length=512\n",
    "                ).to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                    model_predictions.append(probabilities.cpu().numpy()[0])\n",
    "            \n",
    "            test_meta_features.append(np.array(model_predictions))\n",
    "        \n",
    "        # 메타 특성 결합\n",
    "        final_meta_features = np.hstack(test_meta_features)\n",
    "        \n",
    "        # 메타 모델로 최종 예측\n",
    "        final_predictions = self.meta_model.predict(final_meta_features)\n",
    "        final_probabilities = self.meta_model.predict_proba(final_meta_features)\n",
    "        \n",
    "        return final_predictions, final_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce16b92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디바이스 설정 중...\n",
      "디바이스: cuda\n",
      "GPU 메모리: 0.25GB 사용 / 8.00GB 전체 (3.2% 사용)\n"
     ]
    }
   ],
   "source": [
    "print(\"디바이스 설정 중...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"디바이스: {device}\")\n",
    "\n",
    "# GPU 메모리 사용량 체크 함수\n",
    "def print_gpu_utilization():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        usage_percent = (allocated / total) * 100\n",
    "        print(f\"GPU 메모리: {allocated:.2f}GB 사용 / {total:.2f}GB 전체 ({usage_percent:.1f}% 사용)\")\n",
    "    else:\n",
    "        print(\"CUDA 사용 불가\")\n",
    "\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efbbef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Stacking 앙상블 학습 시작!\n",
      "\n",
      "📚 1단계: 베이스 모델들 학습\n",
      "✅ bert_small 토크나이저 로드 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 23523/23523 [00:02<00:00, 8470.98 examples/s]\n",
      "Map: 100%|██████████| 5881/5881 [00:00<00:00, 8625.73 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔥 bert_small 모델 학습 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2850' max='6150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2850/6150 51:47 < 1:00:00, 0.92 it/s, Epoch 11/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.109400</td>\n",
       "      <td>2.962562</td>\n",
       "      <td>0.151165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.493800</td>\n",
       "      <td>1.331347</td>\n",
       "      <td>0.469988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.032500</td>\n",
       "      <td>0.915630</td>\n",
       "      <td>0.584935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.741500</td>\n",
       "      <td>0.674062</td>\n",
       "      <td>0.741200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.548300</td>\n",
       "      <td>0.510139</td>\n",
       "      <td>0.805135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.413800</td>\n",
       "      <td>0.409693</td>\n",
       "      <td>0.848495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.261300</td>\n",
       "      <td>0.338250</td>\n",
       "      <td>0.883013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.216300</td>\n",
       "      <td>0.247987</td>\n",
       "      <td>0.920762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.134700</td>\n",
       "      <td>0.207902</td>\n",
       "      <td>0.937936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.084700</td>\n",
       "      <td>0.154539</td>\n",
       "      <td>0.952389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>0.125329</td>\n",
       "      <td>0.964972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>0.114839</td>\n",
       "      <td>0.967693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.050300</td>\n",
       "      <td>0.097749</td>\n",
       "      <td>0.970753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.041900</td>\n",
       "      <td>0.084994</td>\n",
       "      <td>0.976535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.086451</td>\n",
       "      <td>0.976875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.069478</td>\n",
       "      <td>0.982316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.071738</td>\n",
       "      <td>0.981126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.071074</td>\n",
       "      <td>0.982146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.071298</td>\n",
       "      <td>0.981806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ bert_small 모델 학습 완료!\n",
      "✅ roberta_small 토크나이저 로드 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 23523/23523 [00:02<00:00, 8501.27 examples/s]\n",
      "Map: 100%|██████████| 5881/5881 [00:00<00:00, 6158.13 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔥 roberta_small 모델 학습 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2101' max='3680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2101/3680 33:07 < 24:55, 1.06 it/s, Epoch 11.41/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.181600</td>\n",
       "      <td>3.087374</td>\n",
       "      <td>0.088420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.912500</td>\n",
       "      <td>1.761048</td>\n",
       "      <td>0.420507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.175300</td>\n",
       "      <td>1.050144</td>\n",
       "      <td>0.606359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.774600</td>\n",
       "      <td>0.742543</td>\n",
       "      <td>0.726747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.590900</td>\n",
       "      <td>0.548103</td>\n",
       "      <td>0.807006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.419000</td>\n",
       "      <td>0.423061</td>\n",
       "      <td>0.853256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.278900</td>\n",
       "      <td>0.319827</td>\n",
       "      <td>0.898487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.211600</td>\n",
       "      <td>0.243814</td>\n",
       "      <td>0.923482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.148700</td>\n",
       "      <td>0.206569</td>\n",
       "      <td>0.938106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.103000</td>\n",
       "      <td>0.164750</td>\n",
       "      <td>0.952049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.089400</td>\n",
       "      <td>0.135801</td>\n",
       "      <td>0.962251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.066900</td>\n",
       "      <td>0.118104</td>\n",
       "      <td>0.967352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>0.101860</td>\n",
       "      <td>0.972794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stacking 앙상블 생성 및 학습\n",
    "stacking_ensemble = AdvancedStackingEnsemble(n_folds=5, random_state=42)\n",
    "\n",
    "# 학습 실행 (tokenizer 파라미터 제거)\n",
    "stacking_ensemble.fit(train_data, valid_data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46776aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 예측 중...\n",
      "테스트 데이터: 1780개\n",
      "\n",
      "🔮 Stacking 앙상블 예측 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "roberta_v1 예측: 100%|██████████| 1780/1780 [00:12<00:00, 143.99it/s]\n",
      "roberta_v2 예측: 100%|██████████| 1780/1780 [00:11<00:00, 149.93it/s]\n",
      "roberta_v3 예측: 100%|██████████| 1780/1780 [00:13<00:00, 133.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "\n",
      "평균 예측 신뢰도: 0.9554\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 데이터 예측 중...\")\n",
    "\n",
    "# 테스트 데이터 로드\n",
    "test_df = pd.read_csv(\"./test.csv\")\n",
    "print(f\"테스트 데이터: {len(test_df)}개\")\n",
    "\n",
    "# 테스트 데이터 전처리\n",
    "test_texts = []\n",
    "for _, row in test_df.iterrows():\n",
    "    sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "    text = \" [SEP] \".join(sentences)\n",
    "    test_texts.append(text)\n",
    "\n",
    "# Stacking 앙상블로 예측 (tokenizer 파라미터 제거)\n",
    "final_predictions, final_probabilities = stacking_ensemble.predict(test_texts, device)\n",
    "\n",
    "# 예측 결과를 순열로 변환\n",
    "predicted_orders = []\n",
    "confidences = []\n",
    "\n",
    "for i, pred_label in enumerate(final_predictions):\n",
    "    predicted_order = list(label_to_perm[pred_label])\n",
    "    confidence = np.max(final_probabilities[i])\n",
    "    \n",
    "    predicted_orders.append(predicted_order)\n",
    "    confidences.append(confidence)\n",
    "\n",
    "# 평균 신뢰도 출력\n",
    "avg_confidence = np.mean(confidences)\n",
    "print(f\"\\n평균 예측 신뢰도: {avg_confidence:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2beb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission 파일 생성 중...\n",
      "제출 파일 저장 완료: stacking_ensemble_submission.csv\n",
      "\n",
      "============================================================\n",
      "🏆 고급 Stacking 앙상블 완료!\n",
      "📊 베이스 모델 수: 3개\n",
      "🎯 메타 모델: LightGBM\n",
      "🔮 평균 예측 신뢰도: 95.54%\n",
      "📁 제출 파일: stacking_ensemble_submission.csv\n",
      "============================================================\n",
      "\n",
      "개별 베이스 모델 정보:\n",
      "  모델 1: roberta_v1 (LR: 2e-05, Epochs: 20)\n",
      "  모델 2: roberta_v2 (LR: 1.5e-05, Epochs: 25)\n",
      "  모델 3: roberta_v3 (LR: 3e-05, Epochs: 15)\n"
     ]
    }
   ],
   "source": [
    "print(\"Submission 파일 생성 중...\")\n",
    "\n",
    "# 샘플 제출 파일 로드\n",
    "sample_submission = pd.read_csv(\"./sample_submission.csv\")\n",
    "\n",
    "# 예측 결과를 제출 형식으로 변환\n",
    "for i in range(4):\n",
    "    sample_submission[f\"answer_{i}\"] = [pred[i] for pred in predicted_orders]\n",
    "\n",
    "# 파일 저장\n",
    "submission_filename = \"stacking_ensemble_submission.csv\"\n",
    "sample_submission.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"제출 파일 저장 완료: {submission_filename}\")\n",
    "\n",
    "# 최종 결과 요약\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"🏆 고급 Stacking 앙상블 완료!\")\n",
    "print(f\"📊 베이스 모델 수: {len(stacking_ensemble.base_models)}개\")\n",
    "print(f\"🎯 메타 모델: LightGBM\")\n",
    "print(f\"🔮 평균 예측 신뢰도: {avg_confidence*100:.2f}%\")\n",
    "print(f\"📁 제출 파일: {submission_filename}\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "# 개별 모델 성능도 확인해보기\n",
    "print(f\"\\n개별 베이스 모델 정보:\")\n",
    "for i, model_info in enumerate(stacking_ensemble.base_models):\n",
    "    config = model_info['config']\n",
    "    print(f\"  모델 {i+1}: {config['name']} (LR: {config['learning_rate']}, Epochs: {config['epochs']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e891b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daycon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
