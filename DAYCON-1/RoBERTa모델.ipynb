{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "703e1e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\daycon\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import (\n",
    "    RobertaTokenizer, \n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from itertools import permutations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74934889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 24ê°œì˜ ìˆœì—´ í´ë˜ìŠ¤ ìƒì„±\n",
      "ì˜ˆì‹œ ë§¤í•‘:\n",
      "  ë¼ë²¨ 0: (0, 1, 2, 3)\n",
      "  ë¼ë²¨ 1: (0, 1, 3, 2)\n",
      "  ë¼ë²¨ 2: (0, 2, 1, 3)\n",
      "  ë¼ë²¨ 3: (0, 2, 3, 1)\n",
      "  ë¼ë²¨ 4: (0, 3, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "def create_label_mappings():\n",
    "    \"\"\"24ê°€ì§€ ìˆœì—´ì— ëŒ€í•œ ë¼ë²¨ ë§¤í•‘ ìƒì„±\"\"\"\n",
    "    # ê°€ëŠ¥í•œ ëª¨ë“  ìˆœì—´ ìƒì„± (4! = 24ê°œ)\n",
    "    all_permutations = list(permutations([0, 1, 2, 3]))\n",
    "    \n",
    "    # ìˆœì—´ â†’ ë¼ë²¨ ë²ˆí˜¸ ë§¤í•‘\n",
    "    perm_to_label = {perm: idx for idx, perm in enumerate(all_permutations)}\n",
    "    \n",
    "    # ë¼ë²¨ ë²ˆí˜¸ â†’ ìˆœì—´ ë§¤í•‘ (ì˜ˆì¸¡ì‹œ ì‚¬ìš©)\n",
    "    label_to_perm = {idx: perm for idx, perm in enumerate(all_permutations)}\n",
    "    \n",
    "    print(f\"ì´ {len(all_permutations)}ê°œì˜ ìˆœì—´ í´ë˜ìŠ¤ ìƒì„±\")\n",
    "    print(\"ì˜ˆì‹œ ë§¤í•‘:\")\n",
    "    for i in range(5):\n",
    "        print(f\"  ë¼ë²¨ {i}: {all_permutations[i]}\")\n",
    "    \n",
    "    return perm_to_label, label_to_perm\n",
    "\n",
    "# ìˆœì—´ ë§¤í•‘ ìƒì„±\n",
    "perm_to_label, label_to_perm = create_label_mappings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef94fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_roberta_data(train_df, perm_to_label):\n",
    "    \"\"\"RoBERTaìš© ë°ì´í„° ì¤€ë¹„\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for _, row in train_df.iterrows():\n",
    "        # 4ê°œ ë¬¸ì¥ ì¶”ì¶œ\n",
    "        sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "        \n",
    "        # ì •ë‹µ ìˆœì—´\n",
    "        answer_tuple = tuple([row[f\"answer_{i}\"] for i in range(4)])\n",
    "        \n",
    "        # ë¬¸ì¥ë“¤ì„ [SEP]ë¡œ ì—°ê²°\n",
    "        text = \" [SEP] \".join(sentences)\n",
    "        \n",
    "        # ìˆœì—´ì„ ë¼ë²¨ë¡œ ë³€í™˜\n",
    "        label = perm_to_label[answer_tuple]\n",
    "        \n",
    "        processed_data.append({\n",
    "            \"text\": text,\n",
    "            \"label\": label,\n",
    "            \"original_sentences\": sentences,\n",
    "            \"answer\": answer_tuple\n",
    "        })\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b89decf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "def augment_roberta_data(train_df, perm_to_label, multiplier=2):\n",
    "    \"\"\"ë°ì´í„° ì¦ê°•ìœ¼ë¡œ í•™ìŠµ ë°ì´í„° ëŠ˜ë¦¬ê¸°\"\"\"\n",
    "    augmented_data = []\n",
    "    \n",
    "    # ì›ë³¸ ë°ì´í„° ì²˜ë¦¬\n",
    "    original_data = prepare_roberta_data(train_df, perm_to_label)\n",
    "    augmented_data.extend(original_data)\n",
    "    \n",
    "    # ì¦ê°• ë°ì´í„° ìƒì„±\n",
    "    for _ in range(multiplier - 1):\n",
    "        for _, row in train_df.iterrows():\n",
    "            sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "            original_answer = [row[f\"answer_{i}\"] for i in range(4)]\n",
    "            \n",
    "            # ëœë¤í•˜ê²Œ ë¬¸ì¥ ìˆœì„œ ì„ê¸°\n",
    "            indices = list(range(4))\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            # ìƒˆë¡œìš´ ìˆœì„œì˜ ë¬¸ì¥ë“¤\n",
    "            shuffled_sentences = [sentences[i] for i in indices]\n",
    "            \n",
    "            # ìƒˆë¡œìš´ ì •ë‹µ ê³„ì‚°\n",
    "            new_answer = tuple([indices.index(original_answer[i]) for i in range(4)])\n",
    "            \n",
    "            # í…ìŠ¤íŠ¸ ìƒì„±\n",
    "            text = \" [SEP] \".join(shuffled_sentences)\n",
    "            label = perm_to_label[new_answer]\n",
    "            \n",
    "            augmented_data.append({\n",
    "                \"text\": text,\n",
    "                \"label\": label,\n",
    "                \"original_sentences\": shuffled_sentences,\n",
    "                \"answer\": new_answer\n",
    "            })\n",
    "    \n",
    "    print(f\"ë°ì´í„° ì¦ê°• ì™„ë£Œ: {len(original_data)} â†’ {len(augmented_data)}\")\n",
    "    return augmented_data\n",
    "\n",
    "print(\"ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90071e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ë¡œë“œ ì¤‘...\n",
      "ì›ë³¸ í•™ìŠµ ë°ì´í„°: 7351ê°œ\n",
      "\n",
      "ë°ì´í„° ì¦ê°• ì¤‘...\n",
      "ë°ì´í„° ì¦ê°• ì™„ë£Œ: 7351 â†’ 22053\n",
      "\n",
      "í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• ...\n",
      "í•™ìŠµ ë°ì´í„°: 17642ê°œ\n",
      "ê²€ì¦ ë°ì´í„°: 4411ê°œ\n",
      "\n",
      "ìƒ˜í”Œ ë°ì´í„°:\n",
      "í…ìŠ¤íŠ¸: ë”°ë¼ì„œ, ë‘ ê¸°ìˆ  ëª¨ë‘ë¥¼ ì²´ê³„ì ìœ¼ë¡œ í›ˆë ¨í•˜ëŠ” ê²ƒì´ ê²½ê¸°ë ¥ í–¥ìƒì— ê¸°ì—¬í•œë‹¤. [SEP] íš¨ê³¼ì ì¸ ìŠˆíŒ… ê¸°ìˆ ì€ ë“ì  í™•ë¥ ì„ ë†’ì´ë©°, ìƒëŒ€ ìˆ˜ë¹„ë¥¼ ë¬´ë„ˆëœ¨ë¦¬ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤. [...\n",
      "ë¼ë²¨: 23\n",
      "ì •ë‹µ: (3, 2, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "print(\"ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "train_df = pd.read_csv('./train.csv')\n",
    "print(f\"ì›ë³¸ í•™ìŠµ ë°ì´í„°: {len(train_df)}ê°œ\")\n",
    "\n",
    "# ë°ì´í„° ì¦ê°•\n",
    "print(\"\\në°ì´í„° ì¦ê°• ì¤‘...\")\n",
    "augmented_data = augment_roberta_data(train_df, perm_to_label, multiplier=3)\n",
    "\n",
    "# í•™ìŠµ/ê²€ì¦ ë¶„í• \n",
    "print(\"\\ní•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• ...\")\n",
    "train_data, valid_data = train_test_split(\n",
    "    augmented_data, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=[item[\"label\"] for item in augmented_data]\n",
    ")\n",
    "\n",
    "print(f\"í•™ìŠµ ë°ì´í„°: {len(train_data)}ê°œ\")\n",
    "print(f\"ê²€ì¦ ë°ì´í„°: {len(valid_data)}ê°œ\")\n",
    "\n",
    "# ìƒ˜í”Œ ë°ì´í„° í™•ì¸\n",
    "print(f\"\\nìƒ˜í”Œ ë°ì´í„°:\")\n",
    "print(f\"í…ìŠ¤íŠ¸: {train_data[0]['text'][:100]}...\")\n",
    "print(f\"ë¼ë²¨: {train_data[0]['label']}\")\n",
    "print(f\"ì •ë‹µ: {train_data[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b1c303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...\n",
      "í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ: BertTokenizerFast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\n",
      "ëª¨ë¸: klue/roberta-base\n",
      "ë””ë°”ì´ìŠ¤: cuda\n",
      "íŒŒë¼ë¯¸í„° ìˆ˜: ~125M\n",
      "í´ë˜ìŠ¤ ìˆ˜: 24ê°œ (4! ìˆœì—´)\n"
     ]
    }
   ],
   "source": [
    "print(\"ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...\")\n",
    "\n",
    "# ëª¨ë¸ ì„¤ì •\n",
    "model_name = \"klue/roberta-base\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ë°©ë²• 1: AutoTokenizer ì‚¬ìš© (ê°€ì¥ ì•ˆì „)\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    " # AutoTokenizerê°€ ìë™ìœ¼ë¡œ ì˜¬ë°”ë¥¸ í† í¬ë‚˜ì´ì € í´ë˜ìŠ¤ ì„ íƒ\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ: {type(tokenizer).__name__}\")\n",
    "    \n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=24,  # 24ê°€ì§€ ìˆœì—´\n",
    "    cache_dir='C:/huggingface_cache'\n",
    ")\n",
    "print(\"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "print(f\"ëª¨ë¸: {model_name}\")\n",
    "print(f\"ë””ë°”ì´ìŠ¤: {device}\")\n",
    "print(f\"íŒŒë¼ë¯¸í„° ìˆ˜: ~125M\")\n",
    "print(f\"í´ë˜ìŠ¤ ìˆ˜: 24ê°œ (4! ìˆœì—´)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6efbbef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§• ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17642/17642 [00:02<00:00, 6543.77 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4411/4411 [00:00<00:00, 7160.72 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í¬ë‚˜ì´ì§• ì™„ë£Œ!\n",
      "í† í¬ë‚˜ì´ì§•ëœ í•™ìŠµ ë°ì´í„°: 17642ê°œ\n",
      "í† í¬ë‚˜ì´ì§•ëœ ê²€ì¦ ë°ì´í„°: 4411ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§• ì¤‘...\")\n",
    "\n",
    "# DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "train_df_processed = pd.DataFrame(train_data)\n",
    "valid_df_processed = pd.DataFrame(valid_data)\n",
    "\n",
    "# Dataset ê°ì²´ ìƒì„±\n",
    "train_dataset = Dataset.from_pandas(train_df_processed)\n",
    "valid_dataset = Dataset.from_pandas(valid_df_processed)\n",
    "\n",
    "# í† í¬ë‚˜ì´ì§• í•¨ìˆ˜\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# í† í¬ë‚˜ì´ì§• ì‹¤í–‰\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°\n",
    "train_dataset = train_dataset.remove_columns([\"text\", \"original_sentences\", \"answer\"])\n",
    "valid_dataset = valid_dataset.remove_columns([\"text\", \"original_sentences\", \"answer\"])\n",
    "\n",
    "print(\"í† í¬ë‚˜ì´ì§• ì™„ë£Œ!\")\n",
    "print(f\"í† í¬ë‚˜ì´ì§•ëœ í•™ìŠµ ë°ì´í„°: {len(train_dataset)}ê°œ\")\n",
    "print(f\"í† í¬ë‚˜ì´ì§•ëœ ê²€ì¦ ë°ì´í„°: {len(valid_dataset)}ê°œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29edb899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… êµ¬ë²„ì „ ìµœì í™”ëœ TrainingArguments ì„¤ì • ì™„ë£Œ\n",
      "\n",
      "=== ğŸš€ ìµœì í™”ëœ í•™ìŠµ ì„¤ì • ì™„ë£Œ ===\n",
      "ë°°ì¹˜ í¬ê¸°: 128 \n",
      "ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì : 2 \n",
      "ì‹¤ì œ ë°°ì¹˜ í¬ê¸°: 256 \n",
      "í•™ìŠµë¥ : 2e-05\n",
      "ì—í¬í¬: 20\n",
      "FP16: True\n",
      "GPU ë©”ëª¨ë¦¬: 1.70GB ì‚¬ìš© / 8.00GB ì „ì²´ (21.2% ì‚¬ìš©)\n",
      "\n",
      "ğŸ§ª ìµœì í™”ëœ ì„¤ì • ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ì¤‘...\n",
      "âœ… ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ì„±ê³µ!\n",
      "GPU ë©”ëª¨ë¦¬: 1.70GB ì‚¬ìš© / 8.00GB ì „ì²´ (21.2% ì‚¬ìš©)\n",
      "\n",
      "ğŸ‰ GPU ë©”ëª¨ë¦¬ ì—¬ìœ ë¶„ì„ í™œìš©í•œ ìµœì í™” ì™„ë£Œ!\n",
      "í•™ìŠµì„ ì‹œì‘í•˜ì„¸ìš”! trainer.train()\n"
     ]
    }
   ],
   "source": [
    "# í‰ê°€ ë©”íŠ¸ë¦­ ì •ì˜\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./roberta_results\",\n",
    "    learning_rate=2e-5,\n",
    "    \n",
    "\n",
    "    per_device_train_batch_size=64,        \n",
    "    per_device_eval_batch_size=128,         \n",
    "    gradient_accumulation_steps=2,          \n",
    "    \n",
    "    num_train_epochs=20,\n",
    "    \n",
    "    # í‰ê°€ ë° ì €ì¥ (êµ¬ë²„ì „ ë°©ì‹)\n",
    "    eval_strategy=\"steps\",                  \n",
    "    eval_steps=150,                         \n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=150,                         \n",
    "    save_total_limit=3,                    \n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    # ìµœì í™” ì„¤ì •\n",
    "    fp16=True,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=6,               \n",
    "    \n",
    "    # í•™ìŠµ ìµœì í™”\n",
    "    warmup_steps=150,                       \n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # ë¡œê¹… (ë” ìì£¼)\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=20,                       \n",
    "    report_to=None,\n",
    ")\n",
    "\n",
    "print(\"âœ… êµ¬ë²„ì „ ìµœì í™”ëœ TrainingArguments ì„¤ì • ì™„ë£Œ\")\n",
    "\n",
    "# ë°ì´í„° ì½œë ˆì´í„°\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Trainer ì„¤ì •\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n=== ğŸš€ ìµœì í™”ëœ í•™ìŠµ ì„¤ì • ì™„ë£Œ ===\")\n",
    "print(f\"ë°°ì¹˜ í¬ê¸°: {training_args.per_device_train_batch_size} \")\n",
    "print(f\"ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì : {training_args.gradient_accumulation_steps} \")\n",
    "print(f\"ì‹¤ì œ ë°°ì¹˜ í¬ê¸°: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps} \")\n",
    "print(f\"í•™ìŠµë¥ : {training_args.learning_rate}\")\n",
    "print(f\"ì—í¬í¬: {training_args.num_train_epochs}\")\n",
    "print(f\"FP16: {training_args.fp16}\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬ í•¨ìˆ˜\n",
    "def print_gpu_utilization():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        usage_percent = (allocated / total) * 100\n",
    "        print(f\"GPU ë©”ëª¨ë¦¬: {allocated:.2f}GB ì‚¬ìš© / {total:.2f}GB ì „ì²´ ({usage_percent:.1f}% ì‚¬ìš©)\")\n",
    "    else:\n",
    "        print(\"CUDA ì‚¬ìš© ë¶ˆê°€\")\n",
    "\n",
    "print_gpu_utilization()\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸\n",
    "print(\"\\nğŸ§ª ìµœì í™”ëœ ì„¤ì • ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ì¤‘...\")\n",
    "try:\n",
    "    # ì‘ì€ í…ŒìŠ¤íŠ¸ ë°°ì¹˜ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸\n",
    "    sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "    sample_batch = {k: v.to(model.device) for k, v in sample_batch.items()}\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**sample_batch)\n",
    "    \n",
    "    print(\"âœ… ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ì„±ê³µ!\")\n",
    "    print_gpu_utilization()\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ GPU ë©”ëª¨ë¦¬ ì—¬ìœ ë¶„ì„ í™œìš©í•œ ìµœì í™” ì™„ë£Œ!\")\n",
    "print(f\"í•™ìŠµì„ ì‹œì‘í•˜ì„¸ìš”! trainer.train()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b980a8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ í•™ìŠµ ì‹œì‘...\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/1380 : < :, Epoch 0.01/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# í•™ìŠµ ì‹¤í–‰\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\ní•™ìŠµ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1330d601",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ìµœì¢… ê²€ì¦ ì„±ëŠ¥ í‰ê°€...\")\n",
    "\n",
    "# ê²€ì¦ ë°ì´í„°ë¡œ ì„±ëŠ¥ í‰ê°€\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"\\n=== ìµœì¢… ì„±ëŠ¥ ===\")\n",
    "print(f\"ê²€ì¦ ì •í™•ë„: {eval_results['eval_accuracy']:.4f} ({eval_results['eval_accuracy']*100:.2f}%)\")\n",
    "print(f\"ê²€ì¦ ì†ì‹¤: {eval_results['eval_loss']:.4f}\")\n",
    "\n",
    "# ìƒì„¸ ë¶„ì„ì„ ìœ„í•œ ìƒ˜í”Œ ì˜ˆì¸¡\n",
    "print(f\"\\n=== ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼ ===\")\n",
    "\n",
    "# ëª‡ ê°œ ìƒ˜í”Œì— ëŒ€í•´ ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸\n",
    "sample_data = valid_data[:5]\n",
    "\n",
    "for i, sample in enumerate(sample_data):\n",
    "    # ì˜ˆì¸¡\n",
    "    inputs = tokenizer(\n",
    "        sample['text'],\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_label = torch.argmax(predictions, dim=-1).item()\n",
    "        confidence = predictions[0][predicted_label].item()\n",
    "    \n",
    "    predicted_order = label_to_perm[predicted_label]\n",
    "    true_order = sample['answer']\n",
    "    \n",
    "    print(f\"\\nìƒ˜í”Œ {i+1}:\")\n",
    "    print(f\"ë¬¸ì¥ë“¤: {sample['original_sentences']}\")\n",
    "    print(f\"ì •ë‹µ: {true_order}\")\n",
    "    print(f\"ì˜ˆì¸¡: {predicted_order}\")\n",
    "    print(f\"ì‹ ë¢°ë„: {confidence:.4f}\")\n",
    "    print(f\"ê²°ê³¼: {'âœ“ ì •ë‹µ' if predicted_order == true_order else 'âœ— ì˜¤ë‹µ'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47db32f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nëª¨ë¸ ì €ì¥ ì¤‘...\")\n",
    "\n",
    "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ì¥\n",
    "trainer.save_model(\"./roberta_results/final_model\")\n",
    "tokenizer.save_pretrained(\"./roberta_results/final_model\")\n",
    "\n",
    "print(\"ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\")\n",
    "print(\"ì €ì¥ ìœ„ì¹˜: ./roberta_results/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c989633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence_order(sentences, model, tokenizer, label_to_perm, device):\n",
    "    \"\"\"ë¬¸ì¥ ìˆœì„œ ì˜ˆì¸¡ í•¨ìˆ˜\"\"\"\n",
    "    # ì…ë ¥ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "    text = \" [SEP] \".join(sentences)\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì§•\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "    \n",
    "    # ì˜ˆì¸¡\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_label = torch.argmax(predictions, dim=-1).item()\n",
    "    \n",
    "    # ë¼ë²¨ì„ ìˆœì—´ë¡œ ë³€í™˜\n",
    "    predicted_order = list(label_to_perm[predicted_label])\n",
    "    confidence = predictions[0][predicted_label].item()\n",
    "    \n",
    "    return predicted_order, confidence\n",
    "\n",
    "print(\"ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58fa962",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "test_df = pd.read_csv(\"./test.csv\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ê°œ\")\n",
    "\n",
    "predictions = []\n",
    "confidences = []\n",
    "\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"ì˜ˆì¸¡ ì§„í–‰\"):\n",
    "    sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "    \n",
    "    predicted_order, confidence = predict_sentence_order(\n",
    "        sentences, model, tokenizer, label_to_perm, device\n",
    "    )\n",
    "    \n",
    "    predictions.append(predicted_order)\n",
    "    confidences.append(confidence)\n",
    "\n",
    "# í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°\n",
    "avg_confidence = np.mean(confidences)\n",
    "print(f\"\\ní‰ê·  ì˜ˆì¸¡ ì‹ ë¢°ë„: {avg_confidence:.4f}\")\n",
    "\n",
    "# ì‹ ë¢°ë„ ë¶„í¬ í™•ì¸\n",
    "print(f\"ì‹ ë¢°ë„ ë¶„í¬:\")\n",
    "print(f\"  ìµœì†Œ: {np.min(confidences):.4f}\")\n",
    "print(f\"  ìµœëŒ€: {np.max(confidences):.4f}\")\n",
    "print(f\"  ì¤‘ê°„ê°’: {np.median(confidences):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6346146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submission íŒŒì¼ ìƒì„± ì¤‘...\")\n",
    "\n",
    "# ìƒ˜í”Œ ì œì¶œ íŒŒì¼ ë¡œë“œ\n",
    "sample_submission = pd.read_csv(\"./sample_submission.csv\")\n",
    "\n",
    "# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì œì¶œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "for i in range(4):\n",
    "    sample_submission[f\"answer_{i}\"] = [pred[i] for pred in predictions]\n",
    "\n",
    "# íŒŒì¼ ì €ì¥\n",
    "submission_filename = \"roberta_submission.csv\"\n",
    "sample_submission.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {submission_filename}\")\n",
    "\n",
    "# ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"ğŸ¯ RoBERTa ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“Š ê²€ì¦ ì •í™•ë„: {eval_results['eval_accuracy']*100:.2f}%\")\n",
    "print(f\"ğŸ”® í‰ê·  ì˜ˆì¸¡ ì‹ ë¢°ë„: {avg_confidence*100:.2f}%\")\n",
    "print(f\"ğŸ“ ì œì¶œ íŒŒì¼: {submission_filename}\")\n",
    "print(f\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daycon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
