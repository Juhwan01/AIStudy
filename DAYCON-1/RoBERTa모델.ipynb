{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "703e1e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\daycon\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import (\n",
    "    RobertaTokenizer, \n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from itertools import permutations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74934889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 24개의 순열 클래스 생성\n",
      "예시 매핑:\n",
      "  라벨 0: (0, 1, 2, 3)\n",
      "  라벨 1: (0, 1, 3, 2)\n",
      "  라벨 2: (0, 2, 1, 3)\n",
      "  라벨 3: (0, 2, 3, 1)\n",
      "  라벨 4: (0, 3, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "def create_label_mappings():\n",
    "    \"\"\"24가지 순열에 대한 라벨 매핑 생성\"\"\"\n",
    "    # 가능한 모든 순열 생성 (4! = 24개)\n",
    "    all_permutations = list(permutations([0, 1, 2, 3]))\n",
    "    \n",
    "    # 순열 → 라벨 번호 매핑\n",
    "    perm_to_label = {perm: idx for idx, perm in enumerate(all_permutations)}\n",
    "    \n",
    "    # 라벨 번호 → 순열 매핑 (예측시 사용)\n",
    "    label_to_perm = {idx: perm for idx, perm in enumerate(all_permutations)}\n",
    "    \n",
    "    print(f\"총 {len(all_permutations)}개의 순열 클래스 생성\")\n",
    "    print(\"예시 매핑:\")\n",
    "    for i in range(5):\n",
    "        print(f\"  라벨 {i}: {all_permutations[i]}\")\n",
    "    \n",
    "    return perm_to_label, label_to_perm\n",
    "\n",
    "# 순열 매핑 생성\n",
    "perm_to_label, label_to_perm = create_label_mappings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef94fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_roberta_data(train_df, perm_to_label):\n",
    "    \"\"\"RoBERTa용 데이터 준비\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for _, row in train_df.iterrows():\n",
    "        # 4개 문장 추출\n",
    "        sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "        \n",
    "        # 정답 순열\n",
    "        answer_tuple = tuple([row[f\"answer_{i}\"] for i in range(4)])\n",
    "        \n",
    "        # 문장들을 [SEP]로 연결\n",
    "        text = \" [SEP] \".join(sentences)\n",
    "        \n",
    "        # 순열을 라벨로 변환\n",
    "        label = perm_to_label[answer_tuple]\n",
    "        \n",
    "        processed_data.append({\n",
    "            \"text\": text,\n",
    "            \"label\": label,\n",
    "            \"original_sentences\": sentences,\n",
    "            \"answer\": answer_tuple\n",
    "        })\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b89decf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 전처리 함수 정의 완료!\n"
     ]
    }
   ],
   "source": [
    "def augment_roberta_data(train_df, perm_to_label, multiplier=2):\n",
    "    \"\"\"데이터 증강으로 학습 데이터 늘리기\"\"\"\n",
    "    augmented_data = []\n",
    "    \n",
    "    # 원본 데이터 처리\n",
    "    original_data = prepare_roberta_data(train_df, perm_to_label)\n",
    "    augmented_data.extend(original_data)\n",
    "    \n",
    "    # 증강 데이터 생성\n",
    "    for _ in range(multiplier - 1):\n",
    "        for _, row in train_df.iterrows():\n",
    "            sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "            original_answer = [row[f\"answer_{i}\"] for i in range(4)]\n",
    "            \n",
    "            # 랜덤하게 문장 순서 섞기\n",
    "            indices = list(range(4))\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            # 새로운 순서의 문장들\n",
    "            shuffled_sentences = [sentences[i] for i in indices]\n",
    "            \n",
    "            # 새로운 정답 계산\n",
    "            new_answer = tuple([indices.index(original_answer[i]) for i in range(4)])\n",
    "            \n",
    "            # 텍스트 생성\n",
    "            text = \" [SEP] \".join(shuffled_sentences)\n",
    "            label = perm_to_label[new_answer]\n",
    "            \n",
    "            augmented_data.append({\n",
    "                \"text\": text,\n",
    "                \"label\": label,\n",
    "                \"original_sentences\": shuffled_sentences,\n",
    "                \"answer\": new_answer\n",
    "            })\n",
    "    \n",
    "    print(f\"데이터 증강 완료: {len(original_data)} → {len(augmented_data)}\")\n",
    "    return augmented_data\n",
    "\n",
    "print(\"데이터 전처리 함수 정의 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90071e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로드 중...\n",
      "원본 학습 데이터: 7351개\n",
      "\n",
      "데이터 증강 중...\n",
      "데이터 증강 완료: 7351 → 22053\n",
      "\n",
      "학습/검증 데이터 분할...\n",
      "학습 데이터: 17642개\n",
      "검증 데이터: 4411개\n",
      "\n",
      "샘플 데이터:\n",
      "텍스트: 따라서, 두 기술 모두를 체계적으로 훈련하는 것이 경기력 향상에 기여한다. [SEP] 효과적인 슈팅 기술은 득점 확률을 높이며, 상대 수비를 무너뜨리는 데 중요한 역할을 한다. [...\n",
      "라벨: 23\n",
      "정답: (3, 2, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "print(\"데이터 로드 중...\")\n",
    "train_df = pd.read_csv('./train.csv')\n",
    "print(f\"원본 학습 데이터: {len(train_df)}개\")\n",
    "\n",
    "# 데이터 증강\n",
    "print(\"\\n데이터 증강 중...\")\n",
    "augmented_data = augment_roberta_data(train_df, perm_to_label, multiplier=3)\n",
    "\n",
    "# 학습/검증 분할\n",
    "print(\"\\n학습/검증 데이터 분할...\")\n",
    "train_data, valid_data = train_test_split(\n",
    "    augmented_data, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=[item[\"label\"] for item in augmented_data]\n",
    ")\n",
    "\n",
    "print(f\"학습 데이터: {len(train_data)}개\")\n",
    "print(f\"검증 데이터: {len(valid_data)}개\")\n",
    "\n",
    "# 샘플 데이터 확인\n",
    "print(f\"\\n샘플 데이터:\")\n",
    "print(f\"텍스트: {train_data[0]['text'][:100]}...\")\n",
    "print(f\"라벨: {train_data[0]['label']}\")\n",
    "print(f\"정답: {train_data[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b1c303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 및 토크나이저 로드 중...\n",
      "토크나이저 로드 성공: BertTokenizerFast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델 로드 성공!\n",
      "모델: klue/roberta-base\n",
      "디바이스: cuda\n",
      "파라미터 수: ~125M\n",
      "클래스 수: 24개 (4! 순열)\n"
     ]
    }
   ],
   "source": [
    "print(\"모델 및 토크나이저 로드 중...\")\n",
    "\n",
    "# 모델 설정\n",
    "model_name = \"klue/roberta-base\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 방법 1: AutoTokenizer 사용 (가장 안전)\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    " # AutoTokenizer가 자동으로 올바른 토크나이저 클래스 선택\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"토크나이저 로드 성공: {type(tokenizer).__name__}\")\n",
    "    \n",
    "# 모델 로드\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=24,  # 24가지 순열\n",
    "    cache_dir='C:/huggingface_cache'\n",
    ")\n",
    "print(\"✅ 모델 로드 성공!\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "print(f\"모델: {model_name}\")\n",
    "print(f\"디바이스: {device}\")\n",
    "print(f\"파라미터 수: ~125M\")\n",
    "print(f\"클래스 수: 24개 (4! 순열)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6efbbef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 토크나이징 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 17642/17642 [00:02<00:00, 6543.77 examples/s]\n",
      "Map: 100%|██████████| 4411/4411 [00:00<00:00, 7160.72 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토크나이징 완료!\n",
      "토크나이징된 학습 데이터: 17642개\n",
      "토크나이징된 검증 데이터: 4411개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"데이터셋 토크나이징 중...\")\n",
    "\n",
    "# DataFrame으로 변환\n",
    "train_df_processed = pd.DataFrame(train_data)\n",
    "valid_df_processed = pd.DataFrame(valid_data)\n",
    "\n",
    "# Dataset 객체 생성\n",
    "train_dataset = Dataset.from_pandas(train_df_processed)\n",
    "valid_dataset = Dataset.from_pandas(valid_df_processed)\n",
    "\n",
    "# 토크나이징 함수\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# 토크나이징 실행\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 불필요한 컬럼 제거\n",
    "train_dataset = train_dataset.remove_columns([\"text\", \"original_sentences\", \"answer\"])\n",
    "valid_dataset = valid_dataset.remove_columns([\"text\", \"original_sentences\", \"answer\"])\n",
    "\n",
    "print(\"토크나이징 완료!\")\n",
    "print(f\"토크나이징된 학습 데이터: {len(train_dataset)}개\")\n",
    "print(f\"토크나이징된 검증 데이터: {len(valid_dataset)}개\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29edb899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 구버전 최적화된 TrainingArguments 설정 완료\n",
      "\n",
      "=== 🚀 최적화된 학습 설정 완료 ===\n",
      "배치 크기: 128 \n",
      "그래디언트 누적: 2 \n",
      "실제 배치 크기: 256 \n",
      "학습률: 2e-05\n",
      "에포크: 20\n",
      "FP16: True\n",
      "GPU 메모리: 1.70GB 사용 / 8.00GB 전체 (21.2% 사용)\n",
      "\n",
      "🧪 최적화된 설정 메모리 테스트 중...\n",
      "✅ 메모리 테스트 성공!\n",
      "GPU 메모리: 1.70GB 사용 / 8.00GB 전체 (21.2% 사용)\n",
      "\n",
      "🎉 GPU 메모리 여유분을 활용한 최적화 완료!\n",
      "학습을 시작하세요! trainer.train()\n"
     ]
    }
   ],
   "source": [
    "# 평가 메트릭 정의\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./roberta_results\",\n",
    "    learning_rate=2e-5,\n",
    "    \n",
    "\n",
    "    per_device_train_batch_size=64,        \n",
    "    per_device_eval_batch_size=128,         \n",
    "    gradient_accumulation_steps=2,          \n",
    "    \n",
    "    num_train_epochs=20,\n",
    "    \n",
    "    # 평가 및 저장 (구버전 방식)\n",
    "    eval_strategy=\"steps\",                  \n",
    "    eval_steps=150,                         \n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=150,                         \n",
    "    save_total_limit=3,                    \n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    # 최적화 설정\n",
    "    fp16=True,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=6,               \n",
    "    \n",
    "    # 학습 최적화\n",
    "    warmup_steps=150,                       \n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # 로깅 (더 자주)\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=20,                       \n",
    "    report_to=None,\n",
    ")\n",
    "\n",
    "print(\"✅ 구버전 최적화된 TrainingArguments 설정 완료\")\n",
    "\n",
    "# 데이터 콜레이터\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n=== 🚀 최적화된 학습 설정 완료 ===\")\n",
    "print(f\"배치 크기: {training_args.per_device_train_batch_size} \")\n",
    "print(f\"그래디언트 누적: {training_args.gradient_accumulation_steps} \")\n",
    "print(f\"실제 배치 크기: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps} \")\n",
    "print(f\"학습률: {training_args.learning_rate}\")\n",
    "print(f\"에포크: {training_args.num_train_epochs}\")\n",
    "print(f\"FP16: {training_args.fp16}\")\n",
    "\n",
    "# 메모리 사용량 체크 함수\n",
    "def print_gpu_utilization():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        usage_percent = (allocated / total) * 100\n",
    "        print(f\"GPU 메모리: {allocated:.2f}GB 사용 / {total:.2f}GB 전체 ({usage_percent:.1f}% 사용)\")\n",
    "    else:\n",
    "        print(\"CUDA 사용 불가\")\n",
    "\n",
    "print_gpu_utilization()\n",
    "\n",
    "# 메모리 사용량 테스트\n",
    "print(\"\\n🧪 최적화된 설정 메모리 테스트 중...\")\n",
    "try:\n",
    "    # 작은 테스트 배치로 메모리 사용량 확인\n",
    "    sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "    sample_batch = {k: v.to(model.device) for k, v in sample_batch.items()}\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**sample_batch)\n",
    "    \n",
    "    print(\"✅ 메모리 테스트 성공!\")\n",
    "    print_gpu_utilization()\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 메모리 테스트 실패: {e}\")\n",
    "\n",
    "print(f\"\\n🎉 GPU 메모리 여유분을 활용한 최적화 완료!\")\n",
    "print(f\"학습을 시작하세요! trainer.train()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b980a8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 학습 시작...\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/1380 : < :, Epoch 0.01/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"모델 학습 시작...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 학습 실행\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n학습 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1330d601",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"최종 검증 성능 평가...\")\n",
    "\n",
    "# 검증 데이터로 성능 평가\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"\\n=== 최종 성능 ===\")\n",
    "print(f\"검증 정확도: {eval_results['eval_accuracy']:.4f} ({eval_results['eval_accuracy']*100:.2f}%)\")\n",
    "print(f\"검증 손실: {eval_results['eval_loss']:.4f}\")\n",
    "\n",
    "# 상세 분석을 위한 샘플 예측\n",
    "print(f\"\\n=== 샘플 예측 결과 ===\")\n",
    "\n",
    "# 몇 개 샘플에 대해 예측 결과 확인\n",
    "sample_data = valid_data[:5]\n",
    "\n",
    "for i, sample in enumerate(sample_data):\n",
    "    # 예측\n",
    "    inputs = tokenizer(\n",
    "        sample['text'],\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_label = torch.argmax(predictions, dim=-1).item()\n",
    "        confidence = predictions[0][predicted_label].item()\n",
    "    \n",
    "    predicted_order = label_to_perm[predicted_label]\n",
    "    true_order = sample['answer']\n",
    "    \n",
    "    print(f\"\\n샘플 {i+1}:\")\n",
    "    print(f\"문장들: {sample['original_sentences']}\")\n",
    "    print(f\"정답: {true_order}\")\n",
    "    print(f\"예측: {predicted_order}\")\n",
    "    print(f\"신뢰도: {confidence:.4f}\")\n",
    "    print(f\"결과: {'✓ 정답' if predicted_order == true_order else '✗ 오답'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47db32f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n모델 저장 중...\")\n",
    "\n",
    "# 모델과 토크나이저 저장\n",
    "trainer.save_model(\"./roberta_results/final_model\")\n",
    "tokenizer.save_pretrained(\"./roberta_results/final_model\")\n",
    "\n",
    "print(\"모델 저장 완료!\")\n",
    "print(\"저장 위치: ./roberta_results/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c989633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence_order(sentences, model, tokenizer, label_to_perm, device):\n",
    "    \"\"\"문장 순서 예측 함수\"\"\"\n",
    "    # 입력 텍스트 생성\n",
    "    text = \" [SEP] \".join(sentences)\n",
    "    \n",
    "    # 토크나이징\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "    \n",
    "    # 예측\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_label = torch.argmax(predictions, dim=-1).item()\n",
    "    \n",
    "    # 라벨을 순열로 변환\n",
    "    predicted_order = list(label_to_perm[predicted_label])\n",
    "    confidence = predictions[0][predicted_label].item()\n",
    "    \n",
    "    return predicted_order, confidence\n",
    "\n",
    "print(\"예측 함수 정의 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58fa962",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"테스트 데이터 예측 중...\")\n",
    "\n",
    "# 테스트 데이터 로드\n",
    "test_df = pd.read_csv(\"./test.csv\")\n",
    "print(f\"테스트 데이터: {len(test_df)}개\")\n",
    "\n",
    "predictions = []\n",
    "confidences = []\n",
    "\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"예측 진행\"):\n",
    "    sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "    \n",
    "    predicted_order, confidence = predict_sentence_order(\n",
    "        sentences, model, tokenizer, label_to_perm, device\n",
    "    )\n",
    "    \n",
    "    predictions.append(predicted_order)\n",
    "    confidences.append(confidence)\n",
    "\n",
    "# 평균 신뢰도 계산\n",
    "avg_confidence = np.mean(confidences)\n",
    "print(f\"\\n평균 예측 신뢰도: {avg_confidence:.4f}\")\n",
    "\n",
    "# 신뢰도 분포 확인\n",
    "print(f\"신뢰도 분포:\")\n",
    "print(f\"  최소: {np.min(confidences):.4f}\")\n",
    "print(f\"  최대: {np.max(confidences):.4f}\")\n",
    "print(f\"  중간값: {np.median(confidences):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6346146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submission 파일 생성 중...\")\n",
    "\n",
    "# 샘플 제출 파일 로드\n",
    "sample_submission = pd.read_csv(\"./sample_submission.csv\")\n",
    "\n",
    "# 예측 결과를 제출 형식으로 변환\n",
    "for i in range(4):\n",
    "    sample_submission[f\"answer_{i}\"] = [pred[i] for pred in predictions]\n",
    "\n",
    "# 파일 저장\n",
    "submission_filename = \"roberta_submission.csv\"\n",
    "sample_submission.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"제출 파일 저장 완료: {submission_filename}\")\n",
    "\n",
    "# 결과 요약 출력\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"🎯 RoBERTa 모델 학습 및 예측 완료!\")\n",
    "print(f\"📊 검증 정확도: {eval_results['eval_accuracy']*100:.2f}%\")\n",
    "print(f\"🔮 평균 예측 신뢰도: {avg_confidence*100:.2f}%\")\n",
    "print(f\"📁 제출 파일: {submission_filename}\")\n",
    "print(f\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daycon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
