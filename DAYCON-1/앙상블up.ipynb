{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334da619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import lightgbm as lgb\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F  \n",
    "import safetensors\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from itertools import permutations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Listwise Ranking Loss 정의\n",
    "class ListwiseRankingLoss(nn.Module):\n",
    "    def __init__(self, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def forward(self, logits, labels):\n",
    "        batch_size = logits.size(0)\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx in range(batch_size):\n",
    "            batch_logits = logits[batch_idx] / self.temperature\n",
    "            target_label = labels[batch_idx].item()\n",
    "            target_permutation = self.label_to_permutation(target_label)\n",
    "            \n",
    "            remaining_positions = list(range(4))\n",
    "            listwise_loss = 0\n",
    "            \n",
    "            for pos in range(4):\n",
    "                correct_sentence = target_permutation[pos]\n",
    "                if correct_sentence in remaining_positions:\n",
    "                    position_probs = F.softmax(batch_logits, dim=0)\n",
    "                    listwise_loss += -torch.log(position_probs[target_label] + 1e-8)\n",
    "                    break\n",
    "            \n",
    "            total_loss += listwise_loss\n",
    "        \n",
    "        return total_loss / batch_size\n",
    "    \n",
    "    def label_to_permutation(self, label):\n",
    "        return label_to_perm[label]\n",
    "\n",
    "# 공통 모델 학습 함수\n",
    "def train_single_model(config, train_dataset, valid_dataset, device, model_save_dir=\"./models\"):\n",
    "    \"\"\"단일 모델 학습 및 저장\"\"\"\n",
    "    import os\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n🔥 {config['name']} 모델 학습 시작...\")\n",
    "    \n",
    "    # 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config['model_name'],\n",
    "        cache_dir='C:/huggingface_cache'\n",
    "    )\n",
    "    \n",
    "    # 모델 로드\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config['model_name'],\n",
    "        num_labels=24,\n",
    "        cache_dir='C:/huggingface_cache'\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    # 데이터 토크나이징\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        )\n",
    "    \n",
    "    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_valid = valid_dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    tokenized_train = tokenized_train.remove_columns([\"text\", \"original_sentences\", \"answer\"])\n",
    "    tokenized_valid = tokenized_valid.remove_columns([\"text\", \"original_sentences\", \"answer\"])\n",
    "    \n",
    "    # 학습 설정\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{model_save_dir}/{config['name']}_results\",\n",
    "        learning_rate=config['learning_rate'],\n",
    "        per_device_train_batch_size=config['batch_size'],\n",
    "        per_device_eval_batch_size=64,\n",
    "        num_train_epochs=config['epochs'],\n",
    "        warmup_steps=config['warmup_steps'],\n",
    "        weight_decay=config['weight_decay'],\n",
    "        max_grad_norm=config['max_grad_norm'],\n",
    "        gradient_accumulation_steps=2,\n",
    "        dataloader_pin_memory=True,\n",
    "        dataloader_num_workers=4,\n",
    "        group_by_length=True,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "        dataloader_drop_last=True,\n",
    "        remove_unused_columns=False,\n",
    "        logging_steps=50,\n",
    "        report_to=None,\n",
    "        seed=42,\n",
    "        data_seed=42,\n",
    "    )\n",
    "    \n",
    "    # Listwise Loss 적용 트레이너\n",
    "    class ListwiseTrainer(Trainer):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.listwise_loss = ListwiseRankingLoss(temperature=1.0)\n",
    "        \n",
    "        def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "            labels = inputs.get(\"labels\")\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.get('logits')\n",
    "            loss = self.listwise_loss(logits, labels)\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    # 트레이너 생성\n",
    "    trainer = ListwiseTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(\n",
    "            tokenizer=tokenizer,\n",
    "            padding=True,\n",
    "            max_length=512,\n",
    "            pad_to_multiple_of=8\n",
    "        ),\n",
    "        compute_metrics=lambda eval_pred: {\n",
    "            \"accuracy\": accuracy_score(\n",
    "                eval_pred.label_ids, \n",
    "                np.argmax(eval_pred.predictions, axis=1)\n",
    "            )\n",
    "        },\n",
    "        callbacks=[EarlyStoppingCallback(\n",
    "            early_stopping_patience=config['early_stopping_patience']\n",
    "        )]\n",
    "    )\n",
    "    \n",
    "    print(f\"🎓 {config['name']} 설정:\")\n",
    "    print(f\"   Learning Rate: {config['learning_rate']}\")\n",
    "    print(f\"   Batch Size: {config['batch_size']}\")\n",
    "    print(f\"   Epochs: {config['epochs']}\")\n",
    "    print(f\"   Loss Function: ListMLE (Listwise Ranking)\")\n",
    "    \n",
    "    # 학습 실행\n",
    "    trainer.train()\n",
    "    \n",
    "    # 평가\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"✅ {config['name']} 최종 성능:\")\n",
    "    print(f\"   Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "    \n",
    "    # 모델 저장\n",
    "    model_save_path = f\"{model_save_dir}/{config['name']}_final\"\n",
    "    trainer.save_model(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    \n",
    "    # 설정도 저장\n",
    "    import pickle\n",
    "    config_save_path = f\"{model_save_path}/config.pkl\"\n",
    "    with open(config_save_path, 'wb') as f:\n",
    "        pickle.dump(config, f)\n",
    "    \n",
    "    print(f\"💾 {config['name']} 모델 저장 완료: {model_save_path}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'trainer': trainer,\n",
    "        'config': config,\n",
    "        'save_path': model_save_path,\n",
    "        'final_accuracy': eval_results['eval_accuracy'],\n",
    "        'tokenizer': tokenizer\n",
    "    }\n",
    "\n",
    "# 모델 로드 함수\n",
    "def load_trained_model(model_path, device):\n",
    "    \"\"\"저장된 모델 로드\"\"\"\n",
    "    import pickle\n",
    "    \n",
    "    # 설정 로드\n",
    "    config_path = f\"{model_path}/config.pkl\"\n",
    "    with open(config_path, 'rb') as f:\n",
    "        config = pickle.load(f)\n",
    "    \n",
    "    # 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # 모델 로드\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'config': config,\n",
    "        'tokenizer': tokenizer,\n",
    "        'save_path': model_path\n",
    "    }\n",
    "\n",
    "print(\"✅ 공통 함수 및 클래스 정의 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"데이터 준비 중...\")\n",
    "\n",
    "# 순열 매핑 생성 (기존 코드와 동일)\n",
    "def create_label_mappings():\n",
    "    all_permutations = list(permutations([0, 1, 2, 3]))\n",
    "    perm_to_label = {perm: idx for idx, perm in enumerate(all_permutations)}\n",
    "    label_to_perm = {idx: perm for idx, perm in enumerate(all_permutations)}\n",
    "    return perm_to_label, label_to_perm\n",
    "\n",
    "perm_to_label, label_to_perm = create_label_mappings()\n",
    "\n",
    "# 데이터 증강 함수 (기존과 동일)\n",
    "def augment_roberta_data_advanced(train_df, perm_to_label, multiplier=4):\n",
    "    def prepare_roberta_data(train_df, perm_to_label):\n",
    "        processed_data = []\n",
    "        for _, row in train_df.iterrows():\n",
    "            sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "            answer_tuple = tuple([row[f\"answer_{i}\"] for i in range(4)])\n",
    "            text = \" [SEP] \".join(sentences)\n",
    "            label = perm_to_label[answer_tuple]\n",
    "            \n",
    "            processed_data.append({\n",
    "                \"text\": text,\n",
    "                \"label\": label,\n",
    "                \"original_sentences\": sentences,\n",
    "                \"answer\": answer_tuple\n",
    "            })\n",
    "        return processed_data\n",
    "    \n",
    "    augmented_data = []\n",
    "    original_data = prepare_roberta_data(train_df, perm_to_label)\n",
    "    augmented_data.extend(original_data)\n",
    "    \n",
    "    for aug_round in range(multiplier - 1):\n",
    "        for _, row in train_df.iterrows():\n",
    "            sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "            original_answer = [row[f\"answer_{i}\"] for i in range(4)]\n",
    "            \n",
    "            if aug_round == 0:\n",
    "                indices = list(range(4))\n",
    "                np.random.shuffle(indices)\n",
    "            elif aug_round == 1:\n",
    "                shift = np.random.randint(1, 4)\n",
    "                indices = [(i + shift) % 4 for i in range(4)]\n",
    "            else:\n",
    "                indices = list(range(4))\n",
    "                i, j = np.random.choice(4, 2, replace=False)\n",
    "                indices[i], indices[j] = indices[j], indices[i]\n",
    "            \n",
    "            shuffled_sentences = [sentences[i] for i in indices]\n",
    "            new_answer = tuple([indices.index(original_answer[i]) for i in range(4)])\n",
    "            \n",
    "            text = \" [SEP] \".join(shuffled_sentences)\n",
    "            label = perm_to_label[new_answer]\n",
    "            \n",
    "            augmented_data.append({\n",
    "                \"text\": text,\n",
    "                \"label\": label,\n",
    "                \"original_sentences\": shuffled_sentences,\n",
    "                \"answer\": new_answer\n",
    "            })\n",
    "    \n",
    "    return augmented_data\n",
    "\n",
    "# 데이터 로드 및 증강\n",
    "train_df = pd.read_csv('./train.csv')\n",
    "print(f\"원본 학습 데이터: {len(train_df)}개\")\n",
    "\n",
    "augmented_data = augment_roberta_data_advanced(train_df, perm_to_label, multiplier=4)\n",
    "print(f\"증강 후 데이터: {len(augmented_data)}개\")\n",
    "\n",
    "# 학습/검증 데이터 분할\n",
    "train_data, valid_data = train_test_split(\n",
    "    augmented_data, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=[item[\"label\"] for item in augmented_data]\n",
    ")\n",
    "\n",
    "# 데이터셋 생성\n",
    "train_dataset = Dataset.from_pandas(pd.DataFrame(train_data))\n",
    "valid_dataset = Dataset.from_pandas(pd.DataFrame(valid_data))\n",
    "\n",
    "print(f\"학습 데이터: {len(train_data)}개\")\n",
    "print(f\"검증 데이터: {len(valid_data)}개\")\n",
    "print(\"✅ 데이터 준비 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cdad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"디바이스: {device}\")\n",
    "\n",
    "# ELECTRA Small 설정\n",
    "electra_config = {\n",
    "    'name': 'electra_small_listwise',\n",
    "    'model_name': 'monologg/koelectra-small-v3-discriminator',\n",
    "    'learning_rate': 3e-5,\n",
    "    'epochs': 45,\n",
    "    'batch_size': 64,\n",
    "    'warmup_steps': 300,\n",
    "    'weight_decay': 0.01,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'early_stopping_patience': 8\n",
    "}\n",
    "\n",
    "# ELECTRA 모델 학습 (이 셀만 실행해서 ELECTRA 모델만 학습 가능)\n",
    "try:\n",
    "    electra_model_info = train_single_model(\n",
    "        electra_config, \n",
    "        train_dataset, \n",
    "        valid_dataset, \n",
    "        device, \n",
    "        model_save_dir=\"./saved_models\"\n",
    "    )\n",
    "    print(\"🎉 ELECTRA 모델 학습 완료!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ELECTRA 모델 학습 실패: {e}\")\n",
    "    electra_model_info = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d3f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Base 설정\n",
    "bert_config = {\n",
    "    'name': 'bert_base_listwise',\n",
    "    'model_name': 'klue/bert-base',\n",
    "    'learning_rate': 2e-5,\n",
    "    'epochs': 40,\n",
    "    'batch_size': 32,\n",
    "    'warmup_steps': 400,\n",
    "    'weight_decay': 0.01,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'early_stopping_patience': 8\n",
    "}\n",
    "\n",
    "# BERT 모델 학습 (이 셀만 실행해서 BERT 모델만 학습 가능)\n",
    "try:\n",
    "    bert_model_info = train_single_model(\n",
    "        bert_config, \n",
    "        train_dataset, \n",
    "        valid_dataset, \n",
    "        device, \n",
    "        model_save_dir=\"./saved_models\"\n",
    "    )\n",
    "    print(\"🎉 BERT 모델 학습 완료!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ BERT 모델 학습 실패: {e}\")\n",
    "    bert_model_info = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fdab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoBERTa Small 설정\n",
    "roberta_config = {\n",
    "    'name': 'roberta_small_listwise',\n",
    "    'model_name': 'klue/roberta-small',\n",
    "    'learning_rate': 2e-5,\n",
    "    'epochs': 35,\n",
    "    'batch_size': 48,\n",
    "    'warmup_steps': 300,\n",
    "    'weight_decay': 0.01,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'early_stopping_patience': 6\n",
    "}\n",
    "\n",
    "# RoBERTa 모델 학습 (이 셀만 실행해서 RoBERTa 모델만 학습 가능)\n",
    "try:\n",
    "    roberta_model_info = train_single_model(\n",
    "        roberta_config, \n",
    "        train_dataset, \n",
    "        valid_dataset, \n",
    "        device, \n",
    "        model_save_dir=\"./saved_models\"\n",
    "    )\n",
    "    print(\"🎉 RoBERTa 모델 학습 완료!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ RoBERTa 모델 학습 실패: {e}\")\n",
    "    roberta_model_info = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"저장된 모델들 로드 중...\")\n",
    "\n",
    "# 모델 경로들\n",
    "model_paths = [\n",
    "    \"./saved_models/electra_small_listwise_final\",\n",
    "    \"./saved_models/bert_base_listwise_final\", \n",
    "    \"./saved_models/roberta_small_listwise_final\"\n",
    "]\n",
    "\n",
    "# 모델들 로드\n",
    "loaded_models = []\n",
    "for path in model_paths:\n",
    "    try:\n",
    "        model_info = load_trained_model(path, device)\n",
    "        loaded_models.append(model_info)\n",
    "        print(f\"✅ 모델 로드 성공: {model_info['config']['name']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 모델 로드 실패 ({path}): {e}\")\n",
    "\n",
    "print(f\"총 {len(loaded_models)}개 모델 로드 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a29a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_meta_features(models, data, device, n_folds=5):\n",
    "    \"\"\"메타 특성 생성\"\"\"\n",
    "    print(\"🧠 메타 특성 생성 중...\")\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    meta_features = np.zeros((len(data), len(models) * 24))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
    "        print(f\"  Fold {fold + 1}/{n_folds} 처리 중...\")\n",
    "        \n",
    "        val_data = df.iloc[val_idx]\n",
    "        \n",
    "        for model_idx, model_info in enumerate(models):\n",
    "            model = model_info['model']\n",
    "            tokenizer = model_info['tokenizer']\n",
    "            model.eval()\n",
    "            \n",
    "            fold_predictions = []\n",
    "            \n",
    "            for _, row in val_data.iterrows():\n",
    "                text = row['text']\n",
    "                \n",
    "                inputs = tokenizer(\n",
    "                    text,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    padding=True,\n",
    "                    max_length=512\n",
    "                ).to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                    fold_predictions.append(probabilities.cpu().numpy()[0])\n",
    "            \n",
    "            start_col = model_idx * 24\n",
    "            end_col = (model_idx + 1) * 24\n",
    "            meta_features[val_idx, start_col:end_col] = np.array(fold_predictions)\n",
    "    \n",
    "    print(\"✅ 메타 특성 생성 완료\")\n",
    "    return meta_features\n",
    "\n",
    "# 메타 특성 생성 (모델들이 로드된 후에만 실행)\n",
    "if len(loaded_models) > 0:\n",
    "    meta_features = generate_meta_features(loaded_models, train_data, device)\n",
    "    meta_labels = [item['label'] for item in train_data]\n",
    "    print(f\"메타 특성 형태: {meta_features.shape}\")\n",
    "else:\n",
    "    print(\"❌ 로드된 모델이 없어 메타 특성을 생성할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58446ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_meta_model(meta_features, meta_labels):\n",
    "    \"\"\"메타 모델 학습 (매번 새로 학습)\"\"\"\n",
    "    print(\"🎯 메타 모델 학습 중...\")\n",
    "    \n",
    "    meta_model = lgb.LGBMClassifier(\n",
    "        objective='multiclass',\n",
    "        num_class=24,\n",
    "        boosting_type='gbdt',\n",
    "        num_leaves=31,\n",
    "        learning_rate=0.05,\n",
    "        feature_fraction=0.9,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=5,\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    meta_model.fit(meta_features, meta_labels)\n",
    "    print(\"✅ 메타 모델 학습 완료\")\n",
    "    \n",
    "    return meta_model\n",
    "\n",
    "# 메타 모델 학습 (피클 저장 없이)\n",
    "if len(loaded_models) > 0:\n",
    "    meta_model = train_meta_model(meta_features, meta_labels)\n",
    "    print(\"💡 메타 모델이 메모리에 준비됨 (저장 없음)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76457490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_ensemble(models, meta_model, test_texts, device):\n",
    "    \"\"\"앙상블 예측\"\"\"\n",
    "    print(\"🔮 앙상블 예측 중...\")\n",
    "    \n",
    "    # 각 모델별 예측\n",
    "    test_meta_features = []\n",
    "    \n",
    "    for model_info in models:\n",
    "        model = model_info['model']\n",
    "        tokenizer = model_info['tokenizer']\n",
    "        model.eval()\n",
    "        \n",
    "        model_predictions = []\n",
    "        model_name = model_info['config']['name']\n",
    "        \n",
    "        for text in tqdm(test_texts, desc=f\"{model_name} 예측\"):\n",
    "            inputs = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=512\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                model_predictions.append(probabilities.cpu().numpy()[0])\n",
    "        \n",
    "        test_meta_features.append(np.array(model_predictions))\n",
    "    \n",
    "    # 메타 특성 결합\n",
    "    final_meta_features = np.hstack(test_meta_features)\n",
    "    \n",
    "    # 메타 모델로 최종 예측\n",
    "    final_predictions = meta_model.predict(final_meta_features)\n",
    "    final_probabilities = meta_model.predict_proba(final_meta_features)\n",
    "    \n",
    "    return final_predictions, final_probabilities\n",
    "\n",
    "# 테스트 데이터 로드 및 예측\n",
    "if len(loaded_models) > 0 and 'meta_model' in locals():\n",
    "    print(\"테스트 데이터 예측 시작...\")\n",
    "    \n",
    "    test_df = pd.read_csv(\"./test.csv\")\n",
    "    print(f\"테스트 데이터: {len(test_df)}개\")\n",
    "    \n",
    "    # 테스트 데이터 전처리\n",
    "    test_texts = []\n",
    "    for _, row in test_df.iterrows():\n",
    "        sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "        text = \" [SEP] \".join(sentences)\n",
    "        test_texts.append(text)\n",
    "    \n",
    "    # 예측 실행\n",
    "    final_predictions, final_probabilities = predict_with_ensemble(\n",
    "        loaded_models, meta_model, test_texts, device\n",
    "    )\n",
    "    \n",
    "    # 결과를 순열로 변환\n",
    "    predicted_orders = []\n",
    "    confidences = []\n",
    "    \n",
    "    for i, pred_label in enumerate(final_predictions):\n",
    "        predicted_order = list(label_to_perm[pred_label])\n",
    "        confidence = np.max(final_probabilities[i])\n",
    "        \n",
    "        predicted_orders.append(predicted_order)\n",
    "        confidences.append(confidence)\n",
    "    \n",
    "    avg_confidence = np.mean(confidences)\n",
    "    print(f\"평균 예측 신뢰도: {avg_confidence:.4f}\")\n",
    "    \n",
    "    # 제출 파일 생성\n",
    "    sample_submission = pd.read_csv(\"./sample_submission.csv\")\n",
    "    for i in range(4):\n",
    "        sample_submission[f\"answer_{i}\"] = [pred[i] for pred in predicted_orders]\n",
    "    \n",
    "    submission_filename = \"meta_ensemble_submission.csv\"\n",
    "    sample_submission.to_csv(submission_filename, index=False)\n",
    "    \n",
    "    print(f\"✅ 제출 파일 저장 완료: {submission_filename}\")\n",
    "    print(f\"🏆 메타 모델 앙상블 완료! (모델 수: {len(loaded_models)}개)\")\n",
    "\n",
    "else:\n",
    "    if len(loaded_models) == 0:\n",
    "        print(\"❌ 예측할 모델이 없습니다. 먼저 모델들을 학습해주세요.\")\n",
    "    else:\n",
    "        print(\"❌ 메타 모델이 없습니다. 먼저 메타 모델을 학습해주세요.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
