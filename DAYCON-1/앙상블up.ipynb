{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334da619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê³µí†µ í•¨ìˆ˜ ë° í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import lightgbm as lgb\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F  \n",
    "import safetensors\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from itertools import permutations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Listwise Ranking Loss ì •ì˜ (ìˆ˜ì •ë¨)\n",
    "class ListwiseRankingLoss(nn.Module):\n",
    "    def __init__(self, label_to_perm_dict, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.label_to_perm = label_to_perm_dict  # ğŸ”¥ ë§¤í•‘ì„ í´ë˜ìŠ¤ ë‚´ë¶€ë¡œ!\n",
    "        \n",
    "    def forward(self, logits, labels):\n",
    "        batch_size = logits.size(0)\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx in range(batch_size):\n",
    "            batch_logits = logits[batch_idx] / self.temperature\n",
    "            target_label = labels[batch_idx].item()\n",
    "            target_permutation = self.label_to_perm[target_label]  # ğŸ”¥ self ì‚¬ìš©\n",
    "            \n",
    "            remaining_positions = list(range(4))\n",
    "            listwise_loss = 0\n",
    "            \n",
    "            for pos in range(4):\n",
    "                correct_sentence = target_permutation[pos]\n",
    "                if correct_sentence in remaining_positions:\n",
    "                    position_probs = F.softmax(batch_logits, dim=0)\n",
    "                    listwise_loss += -torch.log(position_probs[target_label] + 1e-8)\n",
    "                    break\n",
    "            \n",
    "            total_loss += listwise_loss\n",
    "        \n",
    "        return total_loss / batch_size\n",
    "\n",
    "# ê³µí†µ ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ (ìˆ˜ì •ë¨)\n",
    "def train_single_model(config, train_dataset, valid_dataset, device, label_to_perm_dict, model_save_dir=\"./models\"):\n",
    "    \"\"\"ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ ë° ì €ì¥\"\"\"\n",
    "    import os\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nğŸ”¥ {config['name']} ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config['model_name'],\n",
    "        cache_dir='C:/huggingface_cache'\n",
    "    )\n",
    "    \n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config['model_name'],\n",
    "        num_labels=24,\n",
    "        cache_dir='C:/huggingface_cache'\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    # ë°ì´í„° í† í¬ë‚˜ì´ì§•\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        )\n",
    "    \n",
    "    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_valid = valid_dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    tokenized_train = tokenized_train.remove_columns([\"text\", \"original_sentences\", \"answer\"])\n",
    "    tokenized_valid = tokenized_valid.remove_columns([\"text\", \"original_sentences\", \"answer\"])\n",
    "    \n",
    "    # í•™ìŠµ ì„¤ì •\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{model_save_dir}/{config['name']}_results\",\n",
    "        learning_rate=config['learning_rate'],\n",
    "        per_device_train_batch_size=config['batch_size'],\n",
    "        per_device_eval_batch_size=64,\n",
    "        num_train_epochs=config['epochs'],\n",
    "        warmup_steps=config['warmup_steps'],\n",
    "        weight_decay=config['weight_decay'],\n",
    "        max_grad_norm=config['max_grad_norm'],\n",
    "        gradient_accumulation_steps=2,\n",
    "        dataloader_pin_memory=True,\n",
    "        dataloader_num_workers=4,\n",
    "        group_by_length=True,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "        dataloader_drop_last=True,\n",
    "        remove_unused_columns=False,\n",
    "        logging_steps=50,\n",
    "        report_to=None,\n",
    "        seed=42,\n",
    "        data_seed=42,\n",
    "    )\n",
    "    \n",
    "    # Listwise Loss ì ìš© íŠ¸ë ˆì´ë„ˆ (ìˆ˜ì •ë¨)\n",
    "    class ListwiseTrainer(Trainer):\n",
    "        def __init__(self, label_to_perm_dict, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.listwise_loss = ListwiseRankingLoss(label_to_perm_dict, temperature=1.0)  # ğŸ”¥ ë§¤í•‘ ì „ë‹¬\n",
    "        \n",
    "        def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "            labels = inputs.get(\"labels\")\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.get('logits')\n",
    "            loss = self.listwise_loss(logits, labels)\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    # íŠ¸ë ˆì´ë„ˆ ìƒì„± (ìˆ˜ì •ë¨)\n",
    "    trainer = ListwiseTrainer(\n",
    "        label_to_perm_dict=label_to_perm_dict,  # ğŸ”¥ ë§¤í•‘ ì „ë‹¬\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_valid,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(\n",
    "            tokenizer=tokenizer,\n",
    "            padding=True,\n",
    "            max_length=512,\n",
    "            pad_to_multiple_of=8\n",
    "        ),\n",
    "        compute_metrics=lambda eval_pred: {\n",
    "            \"accuracy\": accuracy_score(\n",
    "                eval_pred.label_ids, \n",
    "                np.argmax(eval_pred.predictions, axis=1)\n",
    "            )\n",
    "        },\n",
    "        callbacks=[EarlyStoppingCallback(\n",
    "            early_stopping_patience=config['early_stopping_patience']\n",
    "        )]\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ“ {config['name']} ì„¤ì •:\")\n",
    "    print(f\"   Learning Rate: {config['learning_rate']}\")\n",
    "    print(f\"   Batch Size: {config['batch_size']}\")\n",
    "    print(f\"   Epochs: {config['epochs']}\")\n",
    "    print(f\"   Loss Function: ListMLE (Listwise Ranking)\")\n",
    "    \n",
    "    # í•™ìŠµ ì‹¤í–‰\n",
    "    trainer.train()\n",
    "    \n",
    "    # í‰ê°€\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"âœ… {config['name']} ìµœì¢… ì„±ëŠ¥:\")\n",
    "    print(f\"   Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "    \n",
    "    # ëª¨ë¸ ì €ì¥\n",
    "    model_save_path = f\"{model_save_dir}/{config['name']}_final\"\n",
    "    trainer.save_model(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    \n",
    "    # ì„¤ì •ë„ ì €ì¥\n",
    "    import pickle\n",
    "    config_save_path = f\"{model_save_path}/config.pkl\"\n",
    "    with open(config_save_path, 'wb') as f:\n",
    "        pickle.dump(config, f)\n",
    "    \n",
    "    print(f\"ğŸ’¾ {config['name']} ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_save_path}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'trainer': trainer,\n",
    "        'config': config,\n",
    "        'save_path': model_save_path,\n",
    "        'final_accuracy': eval_results['eval_accuracy'],\n",
    "        'tokenizer': tokenizer\n",
    "    }\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜\n",
    "def load_trained_model(model_path, device):\n",
    "    \"\"\"ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "    import pickle\n",
    "    \n",
    "    # ì„¤ì • ë¡œë“œ\n",
    "    config_path = f\"{model_path}/config.pkl\"\n",
    "    with open(config_path, 'rb') as f:\n",
    "        config = pickle.load(f)\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'config': config,\n",
    "        'tokenizer': tokenizer,\n",
    "        'save_path': model_path\n",
    "    }\n",
    "\n",
    "print(\"âœ… ê³µí†µ í•¨ìˆ˜ ë° í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113fbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ì¤€ë¹„ ì¤‘...\n",
      "ì›ë³¸ í•™ìŠµ ë°ì´í„°: 7351ê°œ\n",
      "ì¦ê°• í›„ ë°ì´í„°: 29404ê°œ\n",
      "í•™ìŠµ ë°ì´í„°: 23523ê°œ\n",
      "ê²€ì¦ ë°ì´í„°: 5881ê°œ\n",
      "âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "print(\"ë°ì´í„° ì¤€ë¹„ ì¤‘...\")\n",
    "\n",
    "# ìˆœì—´ ë§¤í•‘ ìƒì„± (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)\n",
    "def create_label_mappings():\n",
    "    all_permutations = list(permutations([0, 1, 2, 3]))\n",
    "    perm_to_label = {perm: idx for idx, perm in enumerate(all_permutations)}\n",
    "    label_to_perm = {idx: perm for idx, perm in enumerate(all_permutations)}\n",
    "    return perm_to_label, label_to_perm\n",
    "\n",
    "perm_to_label, label_to_perm = create_label_mappings()\n",
    "\n",
    "# ë°ì´í„° ì¦ê°• í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)\n",
    "def augment_roberta_data_advanced(train_df, perm_to_label, multiplier=4):\n",
    "    def prepare_roberta_data(train_df, perm_to_label):\n",
    "        processed_data = []\n",
    "        for _, row in train_df.iterrows():\n",
    "            sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "            answer_tuple = tuple([row[f\"answer_{i}\"] for i in range(4)])\n",
    "            text = \" [SEP] \".join(sentences)\n",
    "            label = perm_to_label[answer_tuple]\n",
    "            \n",
    "            processed_data.append({\n",
    "                \"text\": text,\n",
    "                \"label\": label,\n",
    "                \"original_sentences\": sentences,\n",
    "                \"answer\": answer_tuple\n",
    "            })\n",
    "        return processed_data\n",
    "    \n",
    "    augmented_data = []\n",
    "    original_data = prepare_roberta_data(train_df, perm_to_label)\n",
    "    augmented_data.extend(original_data)\n",
    "    \n",
    "    for aug_round in range(multiplier - 1):\n",
    "        for _, row in train_df.iterrows():\n",
    "            sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "            original_answer = [row[f\"answer_{i}\"] for i in range(4)]\n",
    "            \n",
    "            if aug_round == 0:\n",
    "                indices = list(range(4))\n",
    "                np.random.shuffle(indices)\n",
    "            elif aug_round == 1:\n",
    "                shift = np.random.randint(1, 4)\n",
    "                indices = [(i + shift) % 4 for i in range(4)]\n",
    "            else:\n",
    "                indices = list(range(4))\n",
    "                i, j = np.random.choice(4, 2, replace=False)\n",
    "                indices[i], indices[j] = indices[j], indices[i]\n",
    "            \n",
    "            shuffled_sentences = [sentences[i] for i in indices]\n",
    "            new_answer = tuple([indices.index(original_answer[i]) for i in range(4)])\n",
    "            \n",
    "            text = \" [SEP] \".join(shuffled_sentences)\n",
    "            label = perm_to_label[new_answer]\n",
    "            \n",
    "            augmented_data.append({\n",
    "                \"text\": text,\n",
    "                \"label\": label,\n",
    "                \"original_sentences\": shuffled_sentences,\n",
    "                \"answer\": new_answer\n",
    "            })\n",
    "    \n",
    "    return augmented_data\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ ë° ì¦ê°•\n",
    "train_df = pd.read_csv('./train.csv')\n",
    "print(f\"ì›ë³¸ í•™ìŠµ ë°ì´í„°: {len(train_df)}ê°œ\")\n",
    "\n",
    "augmented_data = augment_roberta_data_advanced(train_df, perm_to_label, multiplier=4)\n",
    "print(f\"ì¦ê°• í›„ ë°ì´í„°: {len(augmented_data)}ê°œ\")\n",
    "\n",
    "# í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• \n",
    "train_data, valid_data = train_test_split(\n",
    "    augmented_data, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=[item[\"label\"] for item in augmented_data]\n",
    ")\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "train_dataset = Dataset.from_pandas(pd.DataFrame(train_data))\n",
    "valid_dataset = Dataset.from_pandas(pd.DataFrame(valid_data))\n",
    "\n",
    "print(f\"í•™ìŠµ ë°ì´í„°: {len(train_data)}ê°œ\")\n",
    "print(f\"ê²€ì¦ ë°ì´í„°: {len(valid_data)}ê°œ\")\n",
    "print(\"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cdad2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë””ë°”ì´ìŠ¤: cuda\n",
      "\n",
      "ğŸ”¥ electra_small_listwise ëª¨ë¸ í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23523/23523 [00:03<00:00, 7760.72 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5881/5881 [00:00<00:00, 8425.42 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ electra_small_listwise ì„¤ì •:\n",
      "   Learning Rate: 3e-05\n",
      "   Batch Size: 64\n",
      "   Epochs: 60\n",
      "   Loss Function: ListMLE (Listwise Ranking)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7800' max='11040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7800/11040 2:01:24 < 50:26, 1.07 it/s, Epoch 42/60]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.178100</td>\n",
       "      <td>3.178211</td>\n",
       "      <td>0.040350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.145900</td>\n",
       "      <td>3.174600</td>\n",
       "      <td>0.043784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.035900</td>\n",
       "      <td>2.926933</td>\n",
       "      <td>0.109890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.667600</td>\n",
       "      <td>2.617111</td>\n",
       "      <td>0.119334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.505300</td>\n",
       "      <td>2.444952</td>\n",
       "      <td>0.124313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.361800</td>\n",
       "      <td>2.359980</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.336600</td>\n",
       "      <td>2.322629</td>\n",
       "      <td>0.133070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.284600</td>\n",
       "      <td>2.275527</td>\n",
       "      <td>0.143544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.248600</td>\n",
       "      <td>2.240313</td>\n",
       "      <td>0.193510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.184500</td>\n",
       "      <td>2.159059</td>\n",
       "      <td>0.201065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.099800</td>\n",
       "      <td>2.088173</td>\n",
       "      <td>0.215488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.000700</td>\n",
       "      <td>1.974590</td>\n",
       "      <td>0.262534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.876300</td>\n",
       "      <td>1.881619</td>\n",
       "      <td>0.334135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.783500</td>\n",
       "      <td>1.771734</td>\n",
       "      <td>0.386676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.643600</td>\n",
       "      <td>1.634919</td>\n",
       "      <td>0.396806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.539700</td>\n",
       "      <td>1.510140</td>\n",
       "      <td>0.423420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.427700</td>\n",
       "      <td>1.427703</td>\n",
       "      <td>0.433036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.356000</td>\n",
       "      <td>1.347568</td>\n",
       "      <td>0.456387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.272600</td>\n",
       "      <td>1.286404</td>\n",
       "      <td>0.466861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.226800</td>\n",
       "      <td>1.222565</td>\n",
       "      <td>0.468922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.167300</td>\n",
       "      <td>1.178318</td>\n",
       "      <td>0.525927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.125600</td>\n",
       "      <td>1.121966</td>\n",
       "      <td>0.565076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.052200</td>\n",
       "      <td>1.045618</td>\n",
       "      <td>0.572974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.975900</td>\n",
       "      <td>0.976508</td>\n",
       "      <td>0.587225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.919900</td>\n",
       "      <td>0.915955</td>\n",
       "      <td>0.599588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.857800</td>\n",
       "      <td>0.873917</td>\n",
       "      <td>0.591346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.825300</td>\n",
       "      <td>0.836532</td>\n",
       "      <td>0.607315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.775900</td>\n",
       "      <td>0.791206</td>\n",
       "      <td>0.637878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.757700</td>\n",
       "      <td>0.771841</td>\n",
       "      <td>0.650240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.723900</td>\n",
       "      <td>0.744453</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.702500</td>\n",
       "      <td>0.725318</td>\n",
       "      <td>0.688015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.675200</td>\n",
       "      <td>0.704364</td>\n",
       "      <td>0.694540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.664900</td>\n",
       "      <td>0.674860</td>\n",
       "      <td>0.719265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.625200</td>\n",
       "      <td>0.656230</td>\n",
       "      <td>0.724416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.605500</td>\n",
       "      <td>0.636873</td>\n",
       "      <td>0.729567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.584300</td>\n",
       "      <td>0.611227</td>\n",
       "      <td>0.753262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.558500</td>\n",
       "      <td>0.569379</td>\n",
       "      <td>0.789320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.522100</td>\n",
       "      <td>0.550293</td>\n",
       "      <td>0.804430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.493300</td>\n",
       "      <td>0.529188</td>\n",
       "      <td>0.813530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.475700</td>\n",
       "      <td>0.497970</td>\n",
       "      <td>0.826236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.452500</td>\n",
       "      <td>0.482553</td>\n",
       "      <td>0.832761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.444600</td>\n",
       "      <td>0.454502</td>\n",
       "      <td>0.846326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.388300</td>\n",
       "      <td>0.420273</td>\n",
       "      <td>0.855769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.379400</td>\n",
       "      <td>0.401685</td>\n",
       "      <td>0.865556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.349100</td>\n",
       "      <td>0.387226</td>\n",
       "      <td>0.868990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.355200</td>\n",
       "      <td>0.368668</td>\n",
       "      <td>0.880495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.306900</td>\n",
       "      <td>0.356951</td>\n",
       "      <td>0.882383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.289400</td>\n",
       "      <td>0.331497</td>\n",
       "      <td>0.892514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.278400</td>\n",
       "      <td>0.312518</td>\n",
       "      <td>0.902301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.252800</td>\n",
       "      <td>0.308370</td>\n",
       "      <td>0.902988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.251400</td>\n",
       "      <td>0.295944</td>\n",
       "      <td>0.905391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.230800</td>\n",
       "      <td>0.278889</td>\n",
       "      <td>0.913462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.227500</td>\n",
       "      <td>0.262285</td>\n",
       "      <td>0.922390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.203700</td>\n",
       "      <td>0.255751</td>\n",
       "      <td>0.926683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.246114</td>\n",
       "      <td>0.929087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.181700</td>\n",
       "      <td>0.249620</td>\n",
       "      <td>0.923249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.180600</td>\n",
       "      <td>0.220146</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.163600</td>\n",
       "      <td>0.215228</td>\n",
       "      <td>0.939045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.148700</td>\n",
       "      <td>0.211643</td>\n",
       "      <td>0.936813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.153800</td>\n",
       "      <td>0.200900</td>\n",
       "      <td>0.943853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.131200</td>\n",
       "      <td>0.191795</td>\n",
       "      <td>0.947459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.134600</td>\n",
       "      <td>0.187106</td>\n",
       "      <td>0.949004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.121400</td>\n",
       "      <td>0.184352</td>\n",
       "      <td>0.949004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.115800</td>\n",
       "      <td>0.180118</td>\n",
       "      <td>0.952095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.108400</td>\n",
       "      <td>0.170741</td>\n",
       "      <td>0.952266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.110600</td>\n",
       "      <td>0.164014</td>\n",
       "      <td>0.954327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.162523</td>\n",
       "      <td>0.955357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.098400</td>\n",
       "      <td>0.148845</td>\n",
       "      <td>0.958104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>0.152521</td>\n",
       "      <td>0.957761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.089000</td>\n",
       "      <td>0.144260</td>\n",
       "      <td>0.961023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>0.145678</td>\n",
       "      <td>0.961195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>0.147187</td>\n",
       "      <td>0.961882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.083800</td>\n",
       "      <td>0.143688</td>\n",
       "      <td>0.962397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.071200</td>\n",
       "      <td>0.140854</td>\n",
       "      <td>0.965488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.074300</td>\n",
       "      <td>0.132683</td>\n",
       "      <td>0.967376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.068200</td>\n",
       "      <td>0.130006</td>\n",
       "      <td>0.967033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>0.130968</td>\n",
       "      <td>0.966861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>0.136799</td>\n",
       "      <td>0.966861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='91' max='91' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [91/91 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… electra_small_listwise ìµœì¢… ì„±ëŠ¥:\n",
      "   Accuracy: 0.9674\n",
      "ğŸ’¾ electra_small_listwise ëª¨ë¸ ì €ì¥ ì™„ë£Œ: ./saved_models/electra_small_listwise_final\n",
      "ğŸ‰ ELECTRA ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# GPU ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "# ELECTRA Small ì„¤ì •\n",
    "electra_config = {\n",
    "    'name': 'electra_small_listwise',\n",
    "    'model_name': 'monologg/koelectra-small-v3-discriminator',\n",
    "    'learning_rate': 3e-5,\n",
    "    'epochs': 60,\n",
    "    'batch_size': 64,\n",
    "    'warmup_steps': 300,\n",
    "    'weight_decay': 0.01,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'early_stopping_patience': 3\n",
    "}\n",
    "\n",
    "# ELECTRA ëª¨ë¸ í•™ìŠµ (ì´ ì…€ë§Œ ì‹¤í–‰í•´ì„œ ELECTRA ëª¨ë¸ë§Œ í•™ìŠµ ê°€ëŠ¥)\n",
    "try:\n",
    "    electra_model_info = train_single_model(\n",
    "        electra_config, \n",
    "        train_dataset, \n",
    "        valid_dataset, \n",
    "        device, \n",
    "        model_save_dir=\"./saved_models\"\n",
    "    )\n",
    "    print(\"ğŸ‰ ELECTRA ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ELECTRA ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}\")\n",
    "    electra_model_info = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11d3f1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¥ bert_base_listwise ëª¨ë¸ í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23523/23523 [00:02<00:00, 8105.71 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5881/5881 [00:00<00:00, 8597.86 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ bert_base_listwise ì„¤ì •:\n",
      "   Learning Rate: 2e-05\n",
      "   Batch Size: 32\n",
      "   Epochs: 40\n",
      "   Loss Function: ListMLE (Listwise Ranking)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='14720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2500/14720 1:04:12 < 5:14:04, 0.65 it/s, Epoch 6/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.201100</td>\n",
       "      <td>3.187009</td>\n",
       "      <td>0.047734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.103300</td>\n",
       "      <td>2.961313</td>\n",
       "      <td>0.115385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.262600</td>\n",
       "      <td>1.887782</td>\n",
       "      <td>0.390110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>1.168133</td>\n",
       "      <td>0.505495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.002700</td>\n",
       "      <td>0.911446</td>\n",
       "      <td>0.624657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.835400</td>\n",
       "      <td>0.742969</td>\n",
       "      <td>0.717720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.661200</td>\n",
       "      <td>0.581704</td>\n",
       "      <td>0.778674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.483700</td>\n",
       "      <td>0.483020</td>\n",
       "      <td>0.818510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.440800</td>\n",
       "      <td>0.427066</td>\n",
       "      <td>0.843578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.355100</td>\n",
       "      <td>0.318787</td>\n",
       "      <td>0.894574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.283200</td>\n",
       "      <td>0.271490</td>\n",
       "      <td>0.909169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>0.234188</td>\n",
       "      <td>0.925481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.165100</td>\n",
       "      <td>0.198385</td>\n",
       "      <td>0.938530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.145100</td>\n",
       "      <td>0.153378</td>\n",
       "      <td>0.957246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.098700</td>\n",
       "      <td>0.129743</td>\n",
       "      <td>0.962912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.083100</td>\n",
       "      <td>0.122937</td>\n",
       "      <td>0.965659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.071800</td>\n",
       "      <td>0.122025</td>\n",
       "      <td>0.965659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>0.106568</td>\n",
       "      <td>0.969952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>0.101258</td>\n",
       "      <td>0.971669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.039600</td>\n",
       "      <td>0.088909</td>\n",
       "      <td>0.976648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.079669</td>\n",
       "      <td>0.978880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>0.069326</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.067667</td>\n",
       "      <td>0.982315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.072573</td>\n",
       "      <td>0.979224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>0.066016</td>\n",
       "      <td>0.983173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='91' max='91' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [91/91 00:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… bert_base_listwise ìµœì¢… ì„±ëŠ¥:\n",
      "   Accuracy: 0.9845\n",
      "ğŸ’¾ bert_base_listwise ëª¨ë¸ ì €ì¥ ì™„ë£Œ: ./saved_models/bert_base_listwise_final\n",
      "ğŸ‰ BERT ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# BERT Base ì„¤ì •\n",
    "bert_config = {\n",
    "    'name': 'bert_base_listwise',\n",
    "    'model_name': 'klue/bert-base',\n",
    "    'learning_rate': 2e-5,\n",
    "    'epochs': 40,\n",
    "    'batch_size': 32,\n",
    "    'warmup_steps': 400,\n",
    "    'weight_decay': 0.01,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'early_stopping_patience': 3\n",
    "}\n",
    "\n",
    "# BERT ëª¨ë¸ í•™ìŠµ (ì´ ì…€ë§Œ ì‹¤í–‰í•´ì„œ BERT ëª¨ë¸ë§Œ í•™ìŠµ ê°€ëŠ¥)\n",
    "try:\n",
    "    bert_model_info = train_single_model(\n",
    "        bert_config, \n",
    "        train_dataset, \n",
    "        valid_dataset, \n",
    "        device, \n",
    "        model_save_dir=\"./saved_models\"\n",
    "    )\n",
    "    print(\"ğŸ‰ BERT ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ BERT ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}\")\n",
    "    bert_model_info = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56fdab60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¥ roberta_small_listwise ëª¨ë¸ í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23523/23523 [00:02<00:00, 8323.69 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5881/5881 [00:00<00:00, 9117.77 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ roberta_small_listwise ì„¤ì •:\n",
      "   Learning Rate: 2e-05\n",
      "   Batch Size: 48\n",
      "   Epochs: 35\n",
      "   Loss Function: ListMLE (Listwise Ranking)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2800' max='8575' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2800/8575 59:30 < 2:02:48, 0.78 it/s, Epoch 11/35]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.179600</td>\n",
       "      <td>3.179907</td>\n",
       "      <td>0.043098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.165100</td>\n",
       "      <td>3.035029</td>\n",
       "      <td>0.082933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.369000</td>\n",
       "      <td>2.030219</td>\n",
       "      <td>0.333963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.592000</td>\n",
       "      <td>1.362449</td>\n",
       "      <td>0.495879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.189200</td>\n",
       "      <td>1.067274</td>\n",
       "      <td>0.558551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.955700</td>\n",
       "      <td>0.891644</td>\n",
       "      <td>0.661058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.837800</td>\n",
       "      <td>0.764784</td>\n",
       "      <td>0.726133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.674700</td>\n",
       "      <td>0.651618</td>\n",
       "      <td>0.770604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.586600</td>\n",
       "      <td>0.593504</td>\n",
       "      <td>0.781765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.510800</td>\n",
       "      <td>0.486221</td>\n",
       "      <td>0.830357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.409900</td>\n",
       "      <td>0.422559</td>\n",
       "      <td>0.857486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.372000</td>\n",
       "      <td>0.387777</td>\n",
       "      <td>0.864183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.276500</td>\n",
       "      <td>0.334047</td>\n",
       "      <td>0.889251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.290757</td>\n",
       "      <td>0.905563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.200700</td>\n",
       "      <td>0.247535</td>\n",
       "      <td>0.922905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.178100</td>\n",
       "      <td>0.226139</td>\n",
       "      <td>0.927198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.155500</td>\n",
       "      <td>0.199839</td>\n",
       "      <td>0.939560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.114700</td>\n",
       "      <td>0.193685</td>\n",
       "      <td>0.939045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.122200</td>\n",
       "      <td>0.184503</td>\n",
       "      <td>0.943853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.093900</td>\n",
       "      <td>0.153067</td>\n",
       "      <td>0.951751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.084500</td>\n",
       "      <td>0.149614</td>\n",
       "      <td>0.955357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.079800</td>\n",
       "      <td>0.154015</td>\n",
       "      <td>0.952953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>0.141391</td>\n",
       "      <td>0.957418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.068400</td>\n",
       "      <td>0.141380</td>\n",
       "      <td>0.953640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.058500</td>\n",
       "      <td>0.108615</td>\n",
       "      <td>0.967720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>0.115430</td>\n",
       "      <td>0.964801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.114212</td>\n",
       "      <td>0.966690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.039300</td>\n",
       "      <td>0.126803</td>\n",
       "      <td>0.964457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='91' max='91' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [91/91 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… roberta_small_listwise ìµœì¢… ì„±ëŠ¥:\n",
      "   Accuracy: 0.9681\n",
      "ğŸ’¾ roberta_small_listwise ëª¨ë¸ ì €ì¥ ì™„ë£Œ: ./saved_models/roberta_small_listwise_final\n",
      "ğŸ‰ RoBERTa ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# RoBERTa Small ì„¤ì •\n",
    "roberta_config = {\n",
    "    'name': 'roberta_small_listwise',\n",
    "    'model_name': 'klue/roberta-small',\n",
    "    'learning_rate': 2e-5,\n",
    "    'epochs': 35,\n",
    "    'batch_size': 48,\n",
    "    'warmup_steps': 300,\n",
    "    'weight_decay': 0.01,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'early_stopping_patience': 3\n",
    "}\n",
    "\n",
    "# RoBERTa ëª¨ë¸ í•™ìŠµ (ì´ ì…€ë§Œ ì‹¤í–‰í•´ì„œ RoBERTa ëª¨ë¸ë§Œ í•™ìŠµ ê°€ëŠ¥)\n",
    "try:\n",
    "    roberta_model_info = train_single_model(\n",
    "        roberta_config, \n",
    "        train_dataset, \n",
    "        valid_dataset, \n",
    "        device, \n",
    "        model_save_dir=\"./saved_models\"\n",
    "    )\n",
    "    print(\"ğŸ‰ RoBERTa ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ RoBERTa ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}\")\n",
    "    roberta_model_info = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c7a8a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì €ì¥ëœ ëª¨ë¸ë“¤ ë¡œë“œ ì¤‘...\n",
      "âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: electra_small_listwise\n",
      "âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: bert_base_listwise\n",
      "âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: roberta_small_listwise\n",
      "ì´ 3ê°œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "print(\"ì €ì¥ëœ ëª¨ë¸ë“¤ ë¡œë“œ ì¤‘...\")\n",
    "\n",
    "# ëª¨ë¸ ê²½ë¡œë“¤\n",
    "model_paths = [\n",
    "    \"./saved_models/electra_small_listwise_final\",\n",
    "    \"./saved_models/bert_base_listwise_final\", \n",
    "    \"./saved_models/roberta_small_listwise_final\"\n",
    "]\n",
    "\n",
    "# ëª¨ë¸ë“¤ ë¡œë“œ\n",
    "loaded_models = []\n",
    "for path in model_paths:\n",
    "    try:\n",
    "        model_info = load_trained_model(path, device)\n",
    "        loaded_models.append(model_info)\n",
    "        print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_info['config']['name']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ({path}): {e}\")\n",
    "\n",
    "print(f\"ì´ {len(loaded_models)}ê°œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f9a29a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  ë©”íƒ€ íŠ¹ì„± ìƒì„± ì¤‘...\n",
      "  Fold 1/5 ì²˜ë¦¬ ì¤‘...\n",
      "  Fold 2/5 ì²˜ë¦¬ ì¤‘...\n",
      "  Fold 3/5 ì²˜ë¦¬ ì¤‘...\n",
      "  Fold 4/5 ì²˜ë¦¬ ì¤‘...\n",
      "  Fold 5/5 ì²˜ë¦¬ ì¤‘...\n",
      "âœ… ë©”íƒ€ íŠ¹ì„± ìƒì„± ì™„ë£Œ\n",
      "ë©”íƒ€ íŠ¹ì„± í˜•íƒœ: (23523, 72)\n"
     ]
    }
   ],
   "source": [
    "def generate_meta_features(models, data, device, n_folds=5):\n",
    "    \"\"\"ë©”íƒ€ íŠ¹ì„± ìƒì„±\"\"\"\n",
    "    print(\"ğŸ§  ë©”íƒ€ íŠ¹ì„± ìƒì„± ì¤‘...\")\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    meta_features = np.zeros((len(data), len(models) * 24))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
    "        print(f\"  Fold {fold + 1}/{n_folds} ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        val_data = df.iloc[val_idx]\n",
    "        \n",
    "        for model_idx, model_info in enumerate(models):\n",
    "            model = model_info['model']\n",
    "            tokenizer = model_info['tokenizer']\n",
    "            model.eval()\n",
    "            \n",
    "            fold_predictions = []\n",
    "            \n",
    "            for _, row in val_data.iterrows():\n",
    "                text = row['text']\n",
    "                \n",
    "                inputs = tokenizer(\n",
    "                    text,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    padding=True,\n",
    "                    max_length=512\n",
    "                ).to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                    fold_predictions.append(probabilities.cpu().numpy()[0])\n",
    "            \n",
    "            start_col = model_idx * 24\n",
    "            end_col = (model_idx + 1) * 24\n",
    "            meta_features[val_idx, start_col:end_col] = np.array(fold_predictions)\n",
    "    \n",
    "    print(\"âœ… ë©”íƒ€ íŠ¹ì„± ìƒì„± ì™„ë£Œ\")\n",
    "    return meta_features\n",
    "\n",
    "# ë©”íƒ€ íŠ¹ì„± ìƒì„± (ëª¨ë¸ë“¤ì´ ë¡œë“œëœ í›„ì—ë§Œ ì‹¤í–‰)\n",
    "if len(loaded_models) > 0:\n",
    "    meta_features = generate_meta_features(loaded_models, train_data, device)\n",
    "    meta_labels = [item['label'] for item in train_data]\n",
    "    print(f\"ë©”íƒ€ íŠ¹ì„± í˜•íƒœ: {meta_features.shape}\")\n",
    "else:\n",
    "    print(\"âŒ ë¡œë“œëœ ëª¨ë¸ì´ ì—†ì–´ ë©”íƒ€ íŠ¹ì„±ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58446ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ ë©”íƒ€ ëª¨ë¸ í•™ìŠµ ì¤‘...\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "âœ… ë©”íƒ€ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\n",
      "ğŸ’¡ ë©”íƒ€ ëª¨ë¸ì´ ë©”ëª¨ë¦¬ì— ì¤€ë¹„ë¨ (ì €ì¥ ì—†ìŒ)\n"
     ]
    }
   ],
   "source": [
    "def train_meta_model(meta_features, meta_labels):\n",
    "    \"\"\"ë©”íƒ€ ëª¨ë¸ í•™ìŠµ (ë§¤ë²ˆ ìƒˆë¡œ í•™ìŠµ)\"\"\"\n",
    "    print(\"ğŸ¯ ë©”íƒ€ ëª¨ë¸ í•™ìŠµ ì¤‘...\")\n",
    "    \n",
    "    meta_model = lgb.LGBMClassifier(\n",
    "        objective='multiclass',\n",
    "        num_class=24,\n",
    "        boosting_type='gbdt',\n",
    "        num_leaves=31,\n",
    "        learning_rate=0.05,\n",
    "        feature_fraction=0.9,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=5,\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    meta_model.fit(meta_features, meta_labels)\n",
    "    print(\"âœ… ë©”íƒ€ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\")\n",
    "    \n",
    "    return meta_model\n",
    "\n",
    "# ë©”íƒ€ ëª¨ë¸ í•™ìŠµ (í”¼í´ ì €ì¥ ì—†ì´)\n",
    "if len(loaded_models) > 0:\n",
    "    meta_model = train_meta_model(meta_features, meta_labels)\n",
    "    print(\"ğŸ’¡ ë©”íƒ€ ëª¨ë¸ì´ ë©”ëª¨ë¦¬ì— ì¤€ë¹„ë¨ (ì €ì¥ ì—†ìŒ)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76457490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì‹œì‘...\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„°: 1780ê°œ\n",
      "ğŸ”® ì•™ìƒë¸” ì˜ˆì¸¡ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "electra_small_listwise ì˜ˆì¸¡: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1780/1780 [00:18<00:00, 96.29it/s]\n",
      "bert_base_listwise ì˜ˆì¸¡: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1780/1780 [00:15<00:00, 112.85it/s]\n",
      "roberta_small_listwise ì˜ˆì¸¡: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1780/1780 [00:09<00:00, 184.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "í‰ê·  ì˜ˆì¸¡ ì‹ ë¢°ë„: 0.9697\n",
      "âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: meta_ensemble_submission.csv\n",
      "ğŸ† ë©”íƒ€ ëª¨ë¸ ì•™ìƒë¸” ì™„ë£Œ! (ëª¨ë¸ ìˆ˜: 3ê°œ)\n"
     ]
    }
   ],
   "source": [
    "def predict_with_ensemble(models, meta_model, test_texts, device):\n",
    "    \"\"\"ì•™ìƒë¸” ì˜ˆì¸¡\"\"\"\n",
    "    print(\"ğŸ”® ì•™ìƒë¸” ì˜ˆì¸¡ ì¤‘...\")\n",
    "    \n",
    "    # ê° ëª¨ë¸ë³„ ì˜ˆì¸¡\n",
    "    test_meta_features = []\n",
    "    \n",
    "    for model_info in models:\n",
    "        model = model_info['model']\n",
    "        tokenizer = model_info['tokenizer']\n",
    "        model.eval()\n",
    "        \n",
    "        model_predictions = []\n",
    "        model_name = model_info['config']['name']\n",
    "        \n",
    "        for text in tqdm(test_texts, desc=f\"{model_name} ì˜ˆì¸¡\"):\n",
    "            inputs = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=512\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                model_predictions.append(probabilities.cpu().numpy()[0])\n",
    "        \n",
    "        test_meta_features.append(np.array(model_predictions))\n",
    "    \n",
    "    # ë©”íƒ€ íŠ¹ì„± ê²°í•©\n",
    "    final_meta_features = np.hstack(test_meta_features)\n",
    "    \n",
    "    # ë©”íƒ€ ëª¨ë¸ë¡œ ìµœì¢… ì˜ˆì¸¡\n",
    "    final_predictions = meta_model.predict(final_meta_features)\n",
    "    final_probabilities = meta_model.predict_proba(final_meta_features)\n",
    "    \n",
    "    return final_predictions, final_probabilities\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ë° ì˜ˆì¸¡\n",
    "if len(loaded_models) > 0 and 'meta_model' in locals():\n",
    "    print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì‹œì‘...\")\n",
    "    \n",
    "    test_df = pd.read_csv(\"./test.csv\")\n",
    "    print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ê°œ\")\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬\n",
    "    test_texts = []\n",
    "    for _, row in test_df.iterrows():\n",
    "        sentences = [row[f\"sentence_{i}\"] for i in range(4)]\n",
    "        text = \" [SEP] \".join(sentences)\n",
    "        test_texts.append(text)\n",
    "    \n",
    "    # ì˜ˆì¸¡ ì‹¤í–‰\n",
    "    final_predictions, final_probabilities = predict_with_ensemble(\n",
    "        loaded_models, meta_model, test_texts, device\n",
    "    )\n",
    "    \n",
    "    # ê²°ê³¼ë¥¼ ìˆœì—´ë¡œ ë³€í™˜\n",
    "    predicted_orders = []\n",
    "    confidences = []\n",
    "    \n",
    "    for i, pred_label in enumerate(final_predictions):\n",
    "        predicted_order = list(label_to_perm[pred_label])\n",
    "        confidence = np.max(final_probabilities[i])\n",
    "        \n",
    "        predicted_orders.append(predicted_order)\n",
    "        confidences.append(confidence)\n",
    "    \n",
    "    avg_confidence = np.mean(confidences)\n",
    "    print(f\"í‰ê·  ì˜ˆì¸¡ ì‹ ë¢°ë„: {avg_confidence:.4f}\")\n",
    "    \n",
    "    # ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "    sample_submission = pd.read_csv(\"./sample_submission.csv\")\n",
    "    for i in range(4):\n",
    "        sample_submission[f\"answer_{i}\"] = [pred[i] for pred in predicted_orders]\n",
    "    \n",
    "    submission_filename = \"meta_ensemble_submission.csv\"\n",
    "    sample_submission.to_csv(submission_filename, index=False)\n",
    "    \n",
    "    print(f\"âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {submission_filename}\")\n",
    "    print(f\"ğŸ† ë©”íƒ€ ëª¨ë¸ ì•™ìƒë¸” ì™„ë£Œ! (ëª¨ë¸ ìˆ˜: {len(loaded_models)}ê°œ)\")\n",
    "\n",
    "else:\n",
    "    if len(loaded_models) == 0:\n",
    "        print(\"âŒ ì˜ˆì¸¡í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ë“¤ì„ í•™ìŠµí•´ì£¼ì„¸ìš”.\")\n",
    "    else:\n",
    "        print(\"âŒ ë©”íƒ€ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë©”íƒ€ ëª¨ë¸ì„ í•™ìŠµí•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7086ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daycon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
